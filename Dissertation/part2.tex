\chapter{Решение обратной задачи в пространстве матриц кросс-спектральной плотности}~\label{chapt2}

% План.
% Методы решения обратной задачи для МЭГ/ЭЭГ.
% Сканирующий подход. Нахождение ориентации диполя.
% MUSIC, RAP-MUSIC, бимформеры. Сканирующие методы решения обратной задачи для анализа коннективностей.
% Алгоритм DICS. Модификация DICS для оценки imcoh в пр-ве источников. Wedge MUSIC.
% Методы глолбальной оптимизации.
% Оценка активностей при помощи псевдообратной матрицы.
% Априорная информация о структуре решения. MNE, MinimumCurrent, mxNormMNE.
% Адаптация MxNormMNE к решению обратной задачи для поиска синхронностей.
% Техники ускорения расчета и оптимизации вычислительных ресурсов.
% Active set и связь с MUSIC.
% Критерий останова алгоритма.
% Сокращение размерности пр-ва сенсоров.

\section{Введение}

В предыдущей главе нами была получена система уравнений, связывающая
коэффициенты матрицы кросс-спектральной плотности мощности в пространстве
источников с аналогичными коэффициентами в пространстве сенсоров. Хотя это
уравнение и является линейными относительно неизвестных величин $c_{ij}^{ss}$,
процедура нахождения решения, которое адекватно описывало бы поведение реальных
систем, осложнено тем фактом, что рассматриваемая система уравнений существенно
недоопределена, а следовательно для нее существует бесконечное количество
решений, далеко не все из которых осуществимы для реальных систем
взаимодействующих нейронных ансамблей.

Вообще, система уравнений~\ref{eq:cp_final_re_im} для фиксированных ориентаций
диполей по своей структуре ничем кроме шумового слагаемого не отличается от
систем~\ref{eq:BV_generative_matrix}.
Система уравнений~\ref{eq:BV_generative_matrix} на неизвестные величины $\mathcal{Q}$ также
является линейной и недоопределенной. Некоторые отличия возникают лишь при
рассмотрении свободно ориентированных диполей.

Задачу оценки величин, положений и ориентаций токовых диполей $\mathcal{Q}$ на
основании измерений $\mathcal{B}, \mathcal{V}$ в электрофизиологии принятно
называть \emph{обратной задачей} МЭГ/ЭЭГ.  Как и в случае оценки
кросс-спектральных коэффициентов, решение обратной задачи МЭГ/ЭЭГ не
единственно.  Для выбора какого-либо одного решения $\mathcal{Q}$ по коре
применяют различные эвристики, ограничивающие выбор из бесконечного множества
возможных вариантов.  Как правило, получающееся решение отвечает тому или иному
критерию оптимальности в соответствии с используемой эвристикой, при условии,
что предположения модели выполняются.  Во введении к этой главе мы рассмотрим
основные методы решения обратной задачи для поиска активных токовых диполей на
коре, а затем перейдем к рассмотрению методик для оценки кросс-спектральных
коэффициентов с учетом свободной ориентации.

Все существующие методики решения обратной задачи МЭГ/ЭЭГ можно условно
разделить на два класса.
К первому классу относятся алгоритмы, основанные на
поиске заранее заданного числа эквивалентных токовых диполей, объясняющих измерения
наилучшим образом. При этом оцениваются положения и ориентации этих диполей.
К этому классу относится необходимый нам для дальнейшего изложения алгоритм % Так ли это?
MUSIC (multiple signal classification), а также его модификация RAP-MUSIC
(recursively applied and projected MUSIC), которые условно можно назвать сканирующими.

Второй класс алгоритмов, называемых в литературе <<имиджинговыми>>, ставит задачу
отыскания активности, распределенной сразу по всей коре. Ко второй группе методов
можно условно отнести методы оптимальной пространственной фильтрации, которые
восстанавливают сигнал отдельно для каждой выбранной точки коры в соответствии
с неким локальным критерием оптимальности, а также методы, основанные на выборе
решения с минимальной нормой.

Суть подхода оптимальной фильтрации состоит в том, что для
фиксированной точки внутри объема мозга ставится задача нахождения
пространственного фильтра, оптимизирующего определенную характеристику сигнала,
восстанавливаемого при помощи этого фильтра. В качестве такой характеристики
может выступать, например, отношение сигнал/шум, или же мы можем
руководствоваться критерием минимизации протечки сигнала от других источников в
точку, в которой мы хотим восстановить активность.  Здесь важно отметить, что
конкретный вид решения, полученного в результате оптимизации выбранного
функционала качества будет зависить также от предполагаемой пространственной
структуры шума.

Отметим, что структура восстановленной после применения совокупности найденных
фильтров активации на коре при таком подходе, вообще говоря, субоптимальна с
точки зрения объяснения сигнала, измеренного сенсорами (так как мы
оптимизировали другой функционал качества). Проблема недоопределенности
системы уравнений при этом в некотором смысле остается за скобками, так как для
каждой точки коры решение восстанавливается индивидуально --- без учета вклада
в решение активаций, восстановленных в других точках коры.  Таким образом, для
алгоритмов оптимальной фильтрации найденное решение является
оптимальным в локальном, но не в глобальном смысле.

Задача отыскания активаций, наилучшим образом объясняющих измерения, (т.е.
оптимальных в глобальном смысле) ставится для другого подкласса имиджинговых
методов.
При этом, как уже было отмечено выше, в силу
недоопределенности системы линейных уравнений, связывающих активации на коре с
сигналом на сенсорах, существует бесконечное множество конфигураций источников,
идеально объясняющих померенный сигнал. Тем не менее, среди таких решений в
силу зашумленности измерений а также неточностей при построении прямой модели
реальное распределение активаций (в выделенных точках) коры не содержится, так
как эти <<идеальные>> с точки зрения объяснения измерений решения объясняют в том
числе и шумовую компоненту, которая зачастую оказывается больше или сравнима по
амплитуде с истинной активацией.

Итак, при решении обратной задачи методами глобальной оптимизации существует
две проблемы: бесконечное множество возможных решений и зашумленность
измерений. Чтобы справиться с первой проблемой, для выбора из бесконечного
множества решений пользуются критерием минимальности нормы решения. Иными
словами, среди всех возможных конфигураций первичных токов в объеме (или на
поверхности коры) мозга в качестве решения выбирается такая конфигурация, норма
которой минимальна среди всех возможных.  Условие минимальности нормы в
некотором смысле является следствие принципа бритвы Оккама: мы ищем наиболее
простое решение, удовлетворяющее наблюдениям. Какие решения при этом считать
простыми"--- неочевидный вопрос.  Ответ на него зависит от выбора конкретного
вида нормы, которую мы хотим минимизировать.  Наиболее популярными вариантами
являются $L_2$- и $L_1$-нормы.  Наиболее простой пример минимальной $L_2$-нормы
решения соответствует случаю, когда проблему зашумленности данных мы оставляем
без внимания.  Тогда решение, соответствующее минимуму $L_2$-нормы, получается
применением оператора, соответствующего псевдообратной матрице, взятой для
матрицы прямой модели.  Для $L_1$-нормы ситуация несколько сложнее, так как
решение не может быть получено в явном виде, и требуется численная оптимизация
соответствующего функционала, сводящаяся к задаче линейного программирования.

Рассмотрим теперь, каким образом решается проблема зашумленности данных. Здесь
вновь существует два подхода.  Первый из них используется значительно реже и
состоит в удалении шумовой компоненты из данных посредством сокращенного
сингулярного разложения матрицы прямой модели. Такой подход, например,
использовали авторы, метода Minimum Current Estimate (MCE)~\ref{mce},
порождающего решения с минимальной $L_1$-нормой.

Другой, более популярный подход состоит в использовании тихоновской
регуляризации. В рамках этого подхода норма решения и глобальная ошибка в
объяснении измерений минимизируются совместно, как части одного общего
функционала качества, позволяя тем самым соблюсти баланс между простотой
решения и тем, насколько хорошо оно объясняет измерения, (в том числе,
содержащийся в них шум). Соотношение между <<простотой>> решения и величиной ошибки при
таком подходе можно регулировать настраивая величину метапараметра,
называемого параметром регуляризации. Меняя значение параметра регуляризации
мы стремимся найти такое значение, при котором полученное решение объясняет
только <<полезную>> часть сигнала, записанного сенсорами и полностью игнорирует
шумовую компоненту.

Отметим, что с точки зрения байесовской статистики тихоновская регуляризация
эквивалентна нахождению такого решения обратной задачи, которое соответствует
точке максимума апостериорной плотности вероятности. Минимизируемая норма решения в такой
интерпретации задается априорной плотностью распределения вероятности.

Среди методов, основанных на тихоновской регуляризации, отметим прежде всего
Minimum Norm Estimate (MNE) \ref{mne}, минимизирующий $L_2$-норму решения,
и его вариацию"--- метод dSPM, нормирующий величину восстановленного значения первичного
тока в каждой точке на оцененную величину шума в ней же.

\subsection{Сканирующие алгоритмы}~\label{sect_dics}


Наиболее естественным алгоритма поиска фиксированного числа эквивалентных
токовых диполей (dipole fitting), объясняющих данные наилучшим способом,
является оптимизация их положений и ориентаций методом наименьших квадратов.
Такой подход, однако, обладает существенными недостатками, к которым относится
невыпуклость целевой функции при такой оптимизации,
что приводит к застряванию алгоритма в локальных минимумах.

\subsubsection{MUSIC}
Чтобы обойти эту проблему, Мошер и Лихи предложили использовать для
поиска активных токовых диполей алгоритм MUSIC~\ref{MUSIC, Schmidt}, разработанный
и использовавшийся ранее в радиопеленгации и сонарах.

Рассмотрим подробно суть метода в применении к данным ЭЭГ/МЭГ.
Начнем с рассмотрения порождающей модели сигнала на сенсорах, как мы это
уже делали для оценки фазовой синхронности (см.~\ref{gm_ts}).
На этот раз, однако, заложим в модель возможность свободной ориентации диполей.
От каждого активного токового диполя на коре будем иметь вклад на сенсорах вида:

\begin{equation}
    \mathbf{x}_k(t) =
        \begin{bmatrix}
            |                 & |              & |              \\
            \mathbf{g}_k^1    & \mathbf{g}_k^2 & \mathbf{g}_k^3 \\
            |                 & |              & |
        \end{bmatrix}
        \left(\begin{array}{ccc}
                s_{k,1}(t)\\
                s_{k,2}(t)\\
                s_{k,3}(t)
            \end{array}
        \right)
        % \mathbf{s}_{\xi}(t),
\end{equation},
где $k$"--- индекс токового диполя $s_{k,i}$"--- компоненты соответствующего
дипольного момента,
$\mathbf{g}_k^i$"--- вектора-топографии $k$-го токового диполя
для трех ориентаций тока ($i=1,2,3$).
Тогда вклад от всех активных токовых диполей будет виден на сенсорах как

\begin{equation}
    \mathbf{x}(t) = \mathbf{G} \mathbf{s}(t) + \mathbf{\omega}(t),
    \label{gm_music}
\end{equation}

Здесь вновь $\mathbf{s}(t)$ --- $3n$-мерный вектор-столбец активаций источников,
$\mathbf{x}(t)$ --- $m$-мерный вектор-столбец сигналов на сенсорах,
$t$ --- время, а $\mathbf{G}$ --- $m \times 3n$ матрица линейного
отображения пространства источников пространство сенсоров.

Уравнение \ref{gm_music} задает соответствие между пространством источников и пространством
сенсоров для каждого временного среза $t$.
При условии, что было записано $T$ таких срезов,
можем переписать уравнение~\ref{gm_music} в матричной форме:

\begin{equation}
    \mathbf{X} = \mathbf{G} \mathbf{S} + \mathbf{\Omega}
    \label{gm_music_matrix}
\end{equation}

Заметим, что столбцы матрицы $\mathbf{X}$ порождаются линейными комбинациями
векторов-топографий активных токовых диполей, а значит все возможные конфигурации
наблюдаемого сигнала живут внутри некоторого линейного подпространства линейной оболочки этих
топографий. Это линейное подпространство называется \emph{подпространством сигнала}.
При этом количество активных токовых диполей $r$ задает размерность этого подпространства
Чтобы выделить его, применим к матрице $\mathbf{X}$
сингулярное разложение и зафиксируем первые $r$ левых собственных векторов:

\begin{gather}
    \mathbf{U}, \mathbf{S}, \mathbf{V} = svd(\mathbf{X}) \\
    \mathbf{U} =
        \begin{bmatrix}
            |                 &               & |            \\
            \mathbf{u}_1      & \cdots        & \mathbf{u}_m \\
            |                 &               & |
        \end{bmatrix}\\
    \mathbf{U}_r = 
        \begin{bmatrix}
            |                 &               & |            \\
            \mathbf{u}_1      & \cdots        & \mathbf{u}_r \\
            |                 &               & |
        \end{bmatrix}
    % \mathbf{U}_m = U[:,:m]
\end{gather}

Матрица $\mathbf{U}_r$ называется \emph{матрицей подпространства сигнала};
ее столбцы задают ортонормальный базис этого подпространства.
Чтобы найти все активные токовые диполи, для каждого источника на коре
вычислают \emph{корреляцию подпространств} между подпространством сигнала и
линейной оболочкой трех (двух в случае МЭГ) топографий, соответствующих данному источнику.
Корреляция подпространств $C_k$ вычисляется как максимальное собственное число
произведения матрицы левых собственных векторов для топографий $k$-го источника
и матрицы подпространства сигнала:

\begin{gather}
    \mathbf{U}_{k,g}, \mathbf{S}_{k,g}, \mathbf{V}_{k,g} = svd\left(
            \begin{bmatrix}
                |                 & |              & |              \\
                \mathbf{g}_k^1    & \mathbf{g}_k^2 & \mathbf{g}_k^3 \\
                |                 & |              & |
            \end{bmatrix}
     \right)\\
     C_k = \lambda_{max}(\mathbf{U}_{k,g} \mathbf{U}_r^T)
\end{gather}

Активными считаются такие токовые диполи, для которых величина $C_k$ выше некоторого
заранее порога (авторы статьи \ref{MUSIC} рекомендуют для порога значение 0.95).

Таким образом, для нахождения всех активных токовых диполей необходимо
<<просканировать>> объем или поверхность мозга на предмет источников, для которых
корреляция подпространств с подпространством сигнала превышает некоторое
пороговое значение. 

Одним из сущетственных недостатков подхода MUSIC является потенциальная
сложность в разделении нескольких одновременно активных токовых диполей, а также
в выделении ситуации, когда реальный источник имел не фокальную, а распределенную
структуру. В этом случае в распределении в объеме или по поверхности коры
величин $C_k$ будут присутствовать локальные максимумы, анализ которых требует дополнительных
усилий.


\subsubsection{RAP-MUSIC}
Чтобы справиться с этой проблемой, Мошер и Лихи предложили обобщение метода MUSIC, которое
они назвали Recursively applied and projected MUSIC или RAP-MUSIC.

Идея метода заключается в последовательном применении MUSIC-скана для нахождения
диполя с максимальным значением $С$ с последующей проекцией матрицы данных и топографий оставшихся источников
ортогонально подпространству топографий диполя с максимальным $C_k$, с пердыдущей итерации.

Процедура продолжается до тех пор, пока максимальное по источникам значение корреляции подпространств
на новом шаге не опустится ниже порогового значения.

Описание алгоритма RAP-MUSIC на языке псевдокода приведено в листинге \ref{rap_music_listing}.

\begin{ListingEnv}[!h]
    \begin{lstlisting}[language=Python]
def RAP_MUSIC(X, G, threshold):
    """
    Параметры
    ---------
    X : матрица измерений
    G : матрица прямой модели для свободной ориентации
    threshold : порог корреляции подпространств

    Возвращает
    ----------
    active_dipole_indices : индексы найденных диполей

    """
    # инициализируем пустой список индексов активных диполей
    active_dipole_indices = []

    while True:
        # ищем корреляции подпространств для каждого источника
        C = MUSIC_scan(X, G)
        if max(C) < threshold:
            break
        k = argmax(C)
        active_dipole_indices.append(k) 
        # проецируем от диполей k-го источника
        X, G = project_away_from_k(X, G, k) 

    return active_dipole_indices
    \end{lstlisting}
    \label{rap_music_listing}
\end{ListingEnv}

Такая процедура позволяет исключить вклад в измерения от уже найденных
диполей, последовательно объясняя оставшуюся в данных дисперсию.
Пороговый критерий останова алгоритма гарантирует отсутствие ложноположительных
срабатываний.

Отдельного упоминания для алгоритмов MUSIC и RAP-MUSIC заслуживает выбор
ранга подпространства сигнала. С одной стороны, этот ранг должен совпадать
с количеством реально присутствующих в данных диполей. С другой стороны,
a priori, до применения алгоритма это число не известно. Хорошая новость заключается
в том, что переоценка ранга подпространства сигнала практически не влияет на
качество работы этзих двух алгоритмов,
поэтому авторы статей~\ref{MUSIC, RAP_MUSIC} в качестве общей рекомендации
советуют выбирать значения этого ранга больше реально ожидаемых.

Оба алгоритма, будучи достаточно простыми и вычислительно эффективными,
являются удобным инструментом для решения обратной задачи в первом приближении,
что может быть полезно во-первых для примерного понимания картины распределения
источников, а во-вторых в качестве этапа предобработки при решении ОЗ
для сужения количества вариантов поиска с последующим анализом более
изощренными, но не столь вычислительно быстрыми алгоритмами.

\subsection{Алгоритмы пространственной фильтрации}

Другой класс алгоритмов решения обратной задачи основан на методах оптимальной пространственной
фильтрации сигнала. Различные критерии оптимальности порождают при этом различные алгоритмы.
В общем виде задача построения оптимальных пространственных фильтров для $k$-го источника
сводится к нахождению
матрицы $\mathbf{A}_k$, при умножении которой на матрицу данных $\mathbf{X}$, получается
матрица $\mathbf{S}_k$ размером $3\times T$ временных рядов для трех ортогональных
направлений тока в точке $k$, отвечающая некоторому критерию оптимальности:

\begin{equation}
    f(\mathbf{A}_k \mathbf{X}) \rightarrow opt
\end{equation}

Проведем обзор существующих критериев оптимальности и получающихся из них методов на
основании статьи~\ref{Gross_1999}.

\subsubsection{Пространственный фильтр, оптимизирующий отношение сигнал-шум в заданной точке}

Начнем рассмотрение семейства методов пространственной фильтрации с метода, максимизирующего
отношение сигнал-шум в заданной точке.

Сначала определим отношение сигнал-шум в терминах матриц ковариаций шума и данных.
Пусть $\mathbf{R}_{X,k} = \mathbf{X}\mathbf{X}^T$"--- матрица ковариации сигнала на сенсорах,
приходящего от $k$-го источника
(предполагается что мы уже вычли среднее из временных рядов в матрице $\mathbf{X}$),
а $\mathbf{R}_n$"--- матрица ковариации шума на сенсорах. Тогда отношение
сигнал-шум на сенсорах определяеся как

\begin{equation}
    \rho = \frac{tr(\mathbf{R}_{X,k})}{tr(\mathbf{R}_n)}
\end{equation}

Тогда для отфильтрованного  при помощи фильтра $\mathbf{A}_k$
(матрица размером $m \times 3$) $k$-го источника ОСШ будет выглядеть как

\begin{equation}
    \rho_k = \frac{tr(\mathbf{A}_k \mathbf{R}_{X,k} \mathbf{A}_k^T)}{tr(\mathbf{A}_k \mathbf{R}_n \mathbf{A}_k^T)}
\end{equation}

Тогда задача построения пространственного фильтра $\mathbf{A}_k$,
который максимизирует отношение сигнал-шум
для $k$-го отфильтрованного источника формализуется как

\begin{gather}
    \mathbf{A}_k = \underset{\mathbf{A}_k}{argmax}(\rho_k) =
    \underset{\mathbf{A}_k}{argmax}\left(
        \frac{tr(\mathbf{A}_k \mathbf{R}_{X,k} \mathbf{A}_k^T)}
             {tr(\mathbf{A}_k \mathbf{R}_n \mathbf{A}_k^T)}
         \right)\\
    \label{max_snr_objective}
    s.t.: \mathbf{A}_k\mathbf{A}_k^T = \mathbf{I}_3
\end{gather}

Ограничение необходимо, чтобы фильтр не менял пространственную структуру шума.
Шум будем предполагать пространственно белым с дисперсией $\sigma^2_n$
В общем случае это предположение можно удовлетворить
предварительным отбеливанием данных, применив к ним
оператор $\mathbf{R}_n^{-\frac{1}{2}}$.
% Так как к шумовым источникам при восстановлении активности в
% $k$-ой точке коры относится также любая мозговая активность,
% приходящая из других точек коры, на практике построение опператора
% $\mathbf{R}^{-\frac{1}{2}}$ требует априорного знания о распределении
% источников по коре, и следовательно невозможно. Мы можем, однако,
% использовать определенные приближения для матрицы отбеливания.
% Например, в качестве матрицы ковариации шума зачастую используется
% матрица $\mathbf{G}\mathbf{G}^T$, которая действительно являлась
% бы матрицей ковариации шума, если бы в каждой точке коры присутствовал
% единичный шумовой источник.


В одномерном случае, соответствующем фиксированной ориентации диполя,
матрица фильтров $\mathbf{A}_k$ вырождается в вектор-строку,
а матрица ковариации для источника с индексом $k$ приобретает вид

\begin{equation}
    \mathbf{R}_{X,k} = \sigma^2 \mathbf{g}_k \mathbf{g}_k^T
\end{equation}

где $\sigma^2_k$ --- дисперсия сигнала.
Для матрицы ковариации шума в предположении, что он является пространственно белым, 
будем иметь $\mathbf{R}_n = \mathbf{I}$.
Тогда~\ref{max_snr_objective} в одномерном случае преобразуется к виду

\begin{equation}
    \mathbf{A}_k =
        \underset{\norm{\mathbf{A}_k} = 1}{argmax}\left(
            \frac{\sigma_k^2\norm{\mathbf{A}_k \mathbf{g}_k}^2}
                 {\sigma_n^2\mathbf{A}_k \mathbf{I} \mathbf{A}_k^T}
             \right) = \frac{\sigma_k^2}{\sigma_n^2}\underset{\norm{\mathbf{A}_k} = 1}{argmax}\left(
                \norm{\mathbf{A}_k \mathbf{g}_k}^2
             \right)
\end{equation}


Очевидно, что среди всех векторов единичной длины максимальное значение ОСШ
будет достигаться на векторе, сонаправленном с ориентацией вектора топографии $\mathbf{g}_k$:

\begin{equation}
    \mathbf{A}_k = \frac{\mathbf{g}_k^T}{\norm{\mathbf{g}_k}}
\end{equation}

В случае свободной ориентации диполя матрица $\mathbf{R}_{X,k}$ примет вид

\begin{equation}
    \mathbf{R}_{X,k} = \sigma_k^2 \left[
            \mathbf{g}_k^1, \mathbf{g}_k^2, \mathbf{g}_k^3 \right]
        {\left[\mathbf{g}_k^1, \mathbf{g}_k^2, \mathbf{g}_k^3\right]}^T
\end{equation}

В этом случае любые три вектора (два в случае МЭГ),
образующие ортонормированный базис подпространства, натянутого на
вектора $\mathbf{g}_k^1, \mathbf{g}_k^2, \mathbf{g}_k^3$,
могут быть использованы как компоненты оптимального фильтра.
В частности, для построения фильтра $\mathbf{A}_k$ можем использовать
первые три левых сингулярных вектора матрицы $\mathbf{g}_k$:

\begin{gather}
    \mathbf{U}_{k,g}, \mathbf{S}_{k,g}, \mathbf{V}_{k,g} = svd\left(
            \begin{bmatrix}
                |                 & |              & |              \\
                \mathbf{g}_k^1    & \mathbf{g}_k^2 & \mathbf{g}_k^3 \\
                |                 & |              & |
            \end{bmatrix}
     \right)\\
    \mathbf{A}_k = 
            \begin{bmatrix}
                |                 & |              & |              \\
                \mathbf{U}_{k, g}^1    & \mathbf{U}_{k,g}^2 & \mathbf{U}_{k,g}^3 \\
                |                 & |              & |
            \end{bmatrix}
\end{gather}

Действительно, проекция векторов фильтра
на ортогональное дополнение линейной оболочки векторов
$\mathbf{g}_k^1, \mathbf{g}_k^2, \mathbf{g}_k^3$,
при подсчете произведения $\mathbf{A}_k \mathbf{g}_k \mathbf{g}_k^T \mathbf{A}_k^T$
будет давать нулевой вклад, а значит, так как вектора $\mathbf{A}_k$
фиксированной длины (единичные), они должны целиком принадлежать
линейной оболочке $span(\mathbf{g}_k^1, \mathbf{g}_k^2, \mathbf{g}_k^3)$.
Вместе с тем, из ограничения $\mathbf{A}_k \mathbf{A}_k^T = \mathbf{I}_3$ следует, что
матрица $\mathbf{A}_k$ является матрицей перехода от одного ортонормированного базиса
линейной оболочки $span(\mathbf{g}_k^1, \mathbf{g}_k^2, \mathbf{g}_k^3)$ к другому,
а произведение $\mathbf{A}_k \mathbf{g}_k \mathbf{g}_k^T \mathbf{A}_k^T$
реализует этот переход.
Но след матрицы является инвариантом при переходе от одного ортонормированного базиса к другому,
а значит $tr(\mathbf{A}_k \mathbf{g}_k \mathbf{g}_k^T \mathbf{A}_k^T)$
будет одинаков для любого $\mathbf{A}_k$, строки которого образуют ортонормированный базис
$span(\mathbf{g}_k^1, \mathbf{g}_k^2, \mathbf{g}_k^3)$.

% Установим теперь связь фильтрации, максимизирующей ОСШ, с алгоритмом MUSIC.
Рассмотрим теперь способ получения фильтра, оценивающего истинную ориентацию диполя
и восстанавливающего сигнал вдоль этой ориентации.
После применения полученной матрицы фильтра $\mathbf{A}_k$ к данным,
мы будем получать три временных ряда, соответствующих трем ортогональным направлениям
тока в точке $k$. Для получения фильтра, восстанавливающего единственный временной ряд
вдоль истинной ориентации тока в $k$-ой точке также пользуются критерием максимизации ОСШ.
В этом случае ставится задача отыскания такой ориентации фильтров
$\boldsymbol{\theta}_k = {(\theta^1, \theta^2, \theta^3)}^T$, для
которой отфильтрованный сигнал $\mathbf{S}_k = \boldsymbol{\theta}_k^T \mathbf{A}_k \mathbf{X}$
будет обладать максимальной мощностью среди всех возможных ориентаций.

Для нахождения такой ориентации достаточно взять собственный вектор матрицы
$\mathbf{A}_k \mathbf{X}\mathbf{X}^T \mathbf{A}_k^T$,
соответствующий ее максимальному собственному числу.
Тогда ориентированный фильтр будет выглядеть как

\begin{equation}
    \mathbf{A}_{k}^{or} = \boldsymbol{\theta}^T \mathbf{A}_k
\end{equation}

Заметим, что процедура построения такого фильтра очень похожа на способ
вычисления корреляции подпространств в алгоритме MUSIC\@.
% Полученный вектор в силу свойств сингулярного разложения
% будет иметь максимальную проекцию на подпространство, натянутое на столбцы
% матрицы $\mathbf{X}$, a значит, так как его длина фиксирована и равна единице,
% этот вектор будет составлять минимальный угол с линейной оболочкой
% столбцов $\mathbf{X}$.

\subsubsection{Пространственный фильтр, минимизирующий вклад третих источников в оценку}

Рассмотрим теперь пространственный фильтр,
оптимизационным критерием которого является задача минимизации
вклада активных источников, находящихся в отличных от целевой точках коры.
Этот критерий эквивалентен задаче отыскания фильтра, для котрого отфильтрованный
сигнал будет обладать минимальной энергией при условии, что
сигнал из целевой точки фильтром не искажается.

Математически эта задача в случае фиксированной ориентации диполей запишется как

\begin{equation}
    \Expect{\norm{\mathbf{a}_k \mathbf{X}}^2}
    \rightarrow \underset{\mathbf{a}_k}{\min},
    s.t.: \mathbf{a}_k \mathbf{g}_k = 1
    \label{lcmv_objective_nonreg}
\end{equation}

Выпишем соответствующий лагранжиан c учетом того,
что $\mathbf{R}_{\mathbf{X}} = \Expect{\mathbf{X}\mathbf{X}^T}$~---
это матрица ковариации сигнала на сенсорах:

\begin{equation}
    f_L(\mathbf{a}_k, \lambda) = \mathbf{a}_k \mathbf{R}_{\mathbf{X}} \mathbf{a}_k^T +
                                 \lambda (\mathbf{a}_k \mathbf{g}_k - 1)
\end{equation}


Далее, приравняв производную лагранжиана по $\mathbf{a}_k ^ T$ к нулю, будем иметь:

\begin{equation}
    \frac{\partial f_L}{\partial \mathbf{a}_k} = 
    2 \mathbf{R}_{\mathbf{X}} \mathbf{a}_k^T + \lambda \mathbf{g}_k = 0
\end{equation}

Откуда

\begin{equation}
    \mathbf{a}_k = - \frac{\lambda}{2} \mathbf{g}_k^T\mathbf{R}_{\mathbf{X}}^{-1}
\end{equation}

Используя ограничение, получим

\begin{equation}
    1 = \mathbf{a}_k \mathbf{g}_k =
    - \frac{\lambda}{2} \mathbf{g}_k^T\mathbf{R}_{\mathbf{X}}^{-1} \mathbf{g}_k \implies
    \lambda = -\frac{2}{\mathbf{g}_k^T \mathbf{R}_{\mathbf{X}}^{-1} \mathbf{g}_k}
\end{equation}

Окончательно, будем иметь

\begin{equation}
    \mathbf{a}_k =
    \frac{\mathbf{g}_k^T \mathbf{R}_{\mathbf{X}}^{-1}}{\mathbf{g}_k^T \mathbf{R}_{\mathbf{X}}^{-1} \mathbf{g}_k}
    \label{lcmv_filters}
\end{equation}

Можно показать, что в случае, если количество активных диполей меньше количества сенсоров,
а временные профили активации активных диполей некоррелированны,
такой фильтр будет выдавать нулевые значения для всех точек $k$, не совпадающих
с положениями активных диполей.
Такое поведение фильтра приводит к тому, что в случае, если точки дискретизации коры не
совпадают идеально с положениями активных диполей, такие диполи будут не видны при
фильтрации.
Кроме того, в случае малого количества активных источников такой фильтр
крайне плохо работает при наличии шума в данных, так как матрица $\mathbf{R}_{\mathbf{X}}^{-1}$
в этом случае является вырожденной (ее ранг совпадает с количеством активных диполей).

Чтобы исправить негативные эффекты этого фильтра, пользуются регуляризацией матрицы $\mathbf{R}_{\mathbf{X}}$:

\begin{equation}
    \mathbf{R}_{\mathbf{X}}^{reg} = \mathbf{R}_\mathbf{X} + \alpha \mathbf{I}
\end{equation}

Полученная матрица используется вместо матрицы $\mathbf{R}_\mathbf{X}$ в
уравнении~\ref{lcmv_filters}. Подстраивая коэффициент регуляризации $\alpha$, можно упрявлять степенью
сглаженности пространственной чувствительности фильтров $\mathbf{a}_k$.


Отметим, что решения для фильтров вида~\ref{lcmv_filters},
содержащие матрицу $\mathbf{R}_\mathbf{X}^{reg}$ вместо $\mathbf{R}_\mathbf{X}$ получаются для
видоизмененной версии исходной оптимизационной задачи~\ref{lcmv_objective_nonreg},
которую можно записать как

\begin{equation}
    \Expect{\norm{\mathbf{a}_k \mathbf{X}}^2} + \alpha \norm{\mathbf{a}_k}^2
    \rightarrow \underset{\mathbf{a}_k}{\min},
    s.t.: \mathbf{a}_k \mathbf{g}_k = 1
    \label{lcmv_objective_reg}
\end{equation}

Искомые выражения для фильтров $\mathbf{a}_k$ вновь могут быть получены методом множителей Лагранжа.
Второй член в оптимизируемом функционале~\ref{lcmv_objective_reg} накладывает ограничение на
норму искомых фильтров, не давая ей расти слишком сильно и исключая ту ситуацию,
когда оптимизация приводит к выбору фильтра, лежащего в подпространстве собственных
векторов матрицы $\mathbf{R}_\mathbf{X}$, соответствующих малым (шумовым) собственным числам.
В этом случае угол между фильтром и соответствующей топографией будет близок к $\pi/2$,
и чтобы удовлетворить ограничению, оптимизация будет компенсировать величину проекции
$\mathbf{a}_k$ на $\mathbf{g}_k$ за счет увеличения нормы $\mathbf{a}_k$.

Пространственные фильтры, использующие матрицу ковариации измерений,
обычно принятно называть адаптивными
(так как они не только используют знания о геометрии взаимного расположения коры и массива сенсоров,
но и подстраиваются под текущую активность) или
бимформерами (в дословном переводе~--- формирователь луча),
имея в виду, что они конструируют фильтры,
сфокуссированные на заданных пространственные направлениях, сродни лучу прожектора.

Рассмотрим теперь ту же самую задачу в векторной постановке.
Отметим сначала, что для скоррелированных источников процедура, описанная
выше, будет давать ошибочные значения мощностей сигналов в источниках.
Чтобы убедиться в этом, рассмотрим, как действуют фильтры на вектора прямой модели
в одномерном случае в предположении отсутствия шума,
а также считая, что количество активных источников $q$ меньше количества сенсоров $m$.
В таких предположениях матрица ковариации сигнала на сенсорах $\mathbf{R}_{\mathbf{X}}$
размером $m\times m$ будет иметь ранг $q$, и следовательно будет являться необратимой.
Тем не менее, ее можно приблизить псевдообратной матрицей.
Тогда, так как $\mathbf{R}_\mathbf{X} = \mathbf{G} \mathbf{R}_\mathbf{S} \mathbf{G}^T$

\begin{equation}
    \mathbf{R}_\mathbf{X}^{-1} \approx {(\mathbf{G} \mathbf{R}_\mathbf{X} \mathbf{G}^T)}^\dagger =
    {(\mathbf{G}^\dagger)}^T \mathbf{R}_\mathbf{S}^\dagger \mathbf{G}^\dagger = 
    {(\mathbf{G}^\dagger)}^T \mathbf{R}_\mathbf{S}^{-1} \mathbf{G}^\dagger
\end{equation}

Кроме того, так как $q < m$, псевдообратная для матрицы $\mathbf{G}$ будет также ее левой обратной:

\begin{equation}
    \mathbf{G}^\dagger \mathbf{G} = ({(\mathbf{G}^T \mathbf{G})}^{-1} \mathbf{G}^T) \mathbf{G} = \mathbf{I}
\end{equation}

А значит, для отдельных топографий активных диполей будет выполнено

\begin{equation}
    \mathbf{G}^\dagger \mathbf{g}_k = \mathbf{1}_k
\end{equation}

где $\mathbf{1}_k$ обозначает вектор, у которого на $k$-ом месте стоит единица,
а на всех остальных~--- нули.

Теперь, пользуясь формулой~\ref{lcmv_filters}, можем расписать произведение
$\mathbf{a}_k \mathbf{g}_l$:

\begin{equation}
    \mathbf{a}_k \mathbf{g}_l = \frac{\mathbf{g}_k^T \mathbf{R}_\mathbf{X}^{-1}\mathbf{g}_l}
                                   {\mathbf{g}_k^T \mathbf{R}_\mathbf{X}^{-1} \mathbf{g}_k}=
  \frac{{(\mathbf{G}^\dagger\mathbf{g}_k)}^T \mathbf{R}_\mathbf{S}^{-1} \mathbf{G}^\dagger \mathbf{g}_l}
       {{(\mathbf{G}^\dagger\mathbf{g}_k)}^T \mathbf{R}_\mathbf{S}^{-1} \mathbf{G}^\dagger \mathbf{g}_k}=
       \frac{\mathbf{1}_k \mathbf{R}_\mathbf{S}^{-1} \mathbf{1}_l}
            {\mathbf{1}_k \mathbf{R}_\mathbf{S}^{-1} \mathbf{1}_l} =
            \frac{\mathbf{R}_{\mathbf{S}, kl}^{-1}}
                 {\mathbf{R}_{\mathbf{S}, kk}^{-1}}
\end{equation}

Таким образом, на незашумленных измерениях в случае, если матрица $\mathbf{R}_\mathbf{S}$ диагональна
(а значит диагональна и $\mathbf{R}_\mathbf{S}^{-1}$),
то есть источники нескоррелированны, адаптивный пространственный фильтр
будет идеально восстанавливать источники, так как фильтр, направленный на $k$-ый источник
будет полностью подавлять сигнал, приходящий от всех источников, кроме $k$-го.
В этом случае произведение $\mathbf{a}_k \mathbf{g}_l = \delta_k^l$, где $\delta_k^l$~--- дельта Кронекера.

Вместе с тем, если источники обладают коррелированными временными рядами, неизбежно возникает искажение
восстановленного сигнала за счет протечки в точку, на которую направлен фильтр, сигнала от
этих коррелированных источников.

В случае, если их положение заранее известно, эту ситуацию можно исправить за счет дополнительных
линейных ограничений вида $\mathbf{a}_k \mathbf{g}_l=0$ для ограниченного заранее известного
набора индексов точек коры $l$.

Именно такая ситуация возникает при попытке сформулировать критерий оптимальной
адаптивной фильтрации в векторном случае. Действительно, для каждой точки коры мы будем
иметь идеально скоррелированные источники по трем направлениям ориентации токовых диполей,
так как они будут представлять собой проекции одного и того же вектора плотности тока на три разных
направления. С другой стороны, это один из тех редких случаев, когда мы знаем топорафии
источников с коррелированными временными рядами, поэтому можем явно выписать линейные ограничения
на оптимизируемый функционал. Для $i$-го из трех фильтров будем иметь:

\begin{equation}
    \Expect{\norm{\mathbf{a}_k^i \mathbf{X}}^2}
    \rightarrow \underset{\mathbf{a}_k}{\min},
    s.t.: \mathbf{a}_k^i \mathbf{g}_k^j =  \delta_i^j, j = 1,2,3
    \label{lcmv_opt_crit_components}
\end{equation}

В векторном виде, совокупно для трех фильтров, эта задача может быть переписана в виде


\begin{equation}
    \tr\left\{
        \mathbf{A}_k \mathbf{R}_\mathbf{X}\mathbf{A}_k^T
    \right\} \rightarrow \underset{\mathbf{A}_k}{\min},
    s.t.: \mathbf{A}_k \mathbf{G}_k =  \mathbf{I}
    \label{lcmv_opt_criterion_vec}
\end{equation}

Удобнее, однако, продолжить вывод покомпонентно.
Вновь выпишем лагранжиан для~\ref{lcmv_opt_crit_components} оптимизационной задачи:

\begin{equation}
    f_L(\mathbf{a}_k, \lambda) = \mathbf{a}_k^i \mathbf{R}_\mathbf{X} \mathbf{a}_k^i^T +
    \lambda (\mathbf{g}_k^T \mathbf{a}_k^i^T - \mathbf{1}_i)
\end{equation}

Дифференцируем и приравниваем к нулю:

\begin{equation}
    \frac{\partial f_L}{\partial \mathbf{a}_k^i} =
    2 \mathbf{R}_\mathbf{X} \mathbf{a}_k^i^T + \mathbf{g}_k \lambda^T = 0 \implies
    \mathbf{a}_k^i^T = - \frac{1}{2}\mathbf{R}_\mathbf{X}^{-1} \mathbf{g}_k \lambda^T
\end{equation}

Используя ограничение $\mathbf{g}_k^T \mathbf{a}_k^i^T = \mathbf{1}_i$, выразим $\labmda$:

\begin{equation}
    \mathbf{g}_k^T \mathbf{a}_k^i^T = \mathbf{1}_i =
    - \frac{1}{2}\mathbf{g}_k^T\mathbf{R}_\mathbf{X}^{-1} \mathbf{g}_k \lambda^T \implies
    \lambda^T = - 2{(\mathbf{g}_k^T\mathbf{R}_\mathbf{X}^{-1} \mathbf{g}_k)}^{-1} \mathbf{1}_i
\end{equation}

Окончательно, для $i$-го фильтра будем иметь

\begin{equation}
    \mathbf{a}_k^i^T = \mathbf{R}_\mathbf{X}^{-1} \mathbf{g}_k{(\mathbf{g}_k^T\mathbf{R}_\mathbf{X}^{-1} \mathbf{g}_k)}^{-1} \mathbf{1}_i
    \label{lcmv_filters_vec_comp}
\end{equation}

Или, комбинируя все три фильтра в одной векторной записи и транспонируя:

\begin{equation}
    \mathbf{A}_k =
    {(\mathbf{G}_k^T\mathbf{R}_\mathbf{X}^{-1} \mathbf{G}_k)}^{-1} \mathbf{G}_k^T \mathbf{R}_\mathbf{X}^{-1}
    \label{lcmv_filters_vec}
\end{equation}

\subsubsection{DICS}
\label{DICS_subsection}
Рассмотрим теперь одну важную модификацию метода векторных бимформеров,
разработанную для анализа коннективностей~\ref{DICS}.

Суть метода состоит в построении адаптивных фильтров в частотной области.

Рассмотрим подробно этапы построения DICS-фильтров.
Начнем вновь с рассмотрения порождающей модели сигнала

\begin{equation}
    \mathbf{X} = \mathbf{G} \mathbf{S} + \mathbf{\Omega},
    \label{gm_dics}
\end{equation}


Применим к данным частотно-временное преобразование и посчитаем кросс-спектр по аналогии
с~\ref{gm_timefreq} и~\ref{gm_cp_matr}:

\begin{gather}
    \Cp{x} = \mathbf{G} \Cp{s} \mathbf{G}^T + \Cp{\omega}
    \label{gm_cp_matr}
\end{gather}

Оптимизационная задача метода DICS формализуется следующим образом:

\begin{equation}
    \tr\left\{\mathbf{A}_k \Cp{X} \mathbf{A}_k^T\right\} +
     \alpha \tr\left\{\mathbf{A}_k \mathbf{A}_k^T\right\}
    \rightarrow \underset{\mathbf{A}_k}{\min}
    , s.t.: \mathbf{A}_k \mathbf{G}_k = \mathbf{I}
\end{equation}

Оптимизационный критерий аналогичен критерию из предыдущего раздела
для векторной постановки задачи адаптивной фильтрации,
минимизирующей вклад от третих источников, c добавлением регуляризации матрицы кросс-спектра.

Проводя покомпонентный вывод методом множителей Лагранжа по аналогии с
выводом формул~\ref{lcmv_filters_vec_comp},~\ref{lcmv_filters_vec}, получим
выражения для фильтров в следующем виде:

\begin{equation}
    \mathbf{A}_k =
    {\left(\mathbf{G}_k^T {(\Cp{X} + \alpha \mathbf{I})}^{-1} \mathbf{G}_k\right)}^{-1}
    \mathbf{G}_k^T {\left(\Cp{X} + \alpha \mathbf{I}\right)}^{-1}
\end{equation}

Для получения оценок на элементы матрицы кросс-спектральной плотности для источников
умножим кросс-спетр измерений на полученные матрицы фильтров:

\begin{equation}
    \mathbf{C}^{loose}_{kl} = \mathbf{A}_k \Cp{X} \mathbf{A}_l^H
\end{equation}

Полученная матрица $3\times 3$, $\mathbf{C}^{loose}_{kl}$
содержит оценки элементов кросс-спектра источников по трем ортогональным направлениям.
Оценочное значения элемента кросс-спектра для истинной ориентации диполя
выбирается как след матрицы $\mathbf{C}^{loose}_{kl}$:

\begin{equation}
    \Cp{S}_{kl} = \tr\left\{\mathbf{A}_k \Cp{X} \mathbf{A}_l^H\right\}
\end{equation}
В случае, если максимальное собственное значение матрицы $\mathbf{C}^{loose}_{kl}$
сильно превосходит два других, в качестве оценки на $\Cp{S}_{kl}$ можно брать
максимальное собственное число матрицы $\mathbf{C}^{loose}_{kl}$, что соответствует
случаю, когда направления диполей в точках $k$ и $l$ приближенно оставались неизменными
за время оценки матрицы $\Cp{X}$.

Окончательно, для двух источников $k$, $l$ будем иметь оценку квадрата когерентности вида

\begin{equation}
    \coh{(k, l)}^2 = \frac{\Cp{S}_{kl}}{\sqrt{\Cp{S}_{kk} \Cp{S}_{ll}}}
\end{equation}

Отметим, что такой метод оценки функциональной коннективности, хотя и является на сегодняшний
день одним из самых популярных в нейронаучном сообществе, никак не борется с проблемой
протечки сигнала, а следовательно подвержен ложноположительным срабатыываниям и сложностью
в аккуратной интерпретации полученного распределения коннективнотей.

\section{Методы глобальной оптимизации (методы минимальной нормы)}

К этой группе методов мы относим такие подходы решения обратной задачи,
которые основываются на минимизации глобального расхождения между измеренными данными
и данными, которые получаются при отображении на сенсоры восстановленных источников.
Отметим, что некоторые методы из этой группы также могут быть классифицированы как
пространственные фильтры, однако нам для дальнейшего изложения удобнее причислять их к
текущей группе.

Начнем рассмотрение с самого простого метода этого семейства~--- восстановления источников методом
псевдообратной матрицы.

\subsection{Умножение на псевдообратную матрицу}

Начнем с порождающей модели данных, предположив, что шума нет: 

\begin{equation}
    \vx(t) = \mG \vs(t) + \vomega(t)
    \label{gm_pinv}
\end{equation}

В случае, если количество активных источников меньше количества сенсоров,
а положения (а значит и топографии) этих источников известны, система
уравнений выше является переопределенной и может быть легко решена в смысле
минимизации среднеквадратичной ошибки.
Решение в этом случае широко известно и может быть выписано в явном виде
через псевдообратную матрицу, которая выписывается в явном виде как
$\mG^\dagger = {(\mG^T\mG)}^{-1}\mG^T$, поскольку $\mG^T\mG$ обратима:

\begin{equation*}
    \vs = \mG^\dagger\vx
\end{equation*}

На практике, однако, в задаче картирования мы имеем дело с противоположной ситуацией,
когда количество и положение реальных источников заранее неизвестно, и мы
покрываем кору равномерной сеткой точек, в каждой из которых мы пытаемся восстановить активность,
возможно нулевую. В этом случае количество неизвестных компонент вектора $\vs$ в уравнении~\ref{gm_pinv}
оказывается много больше количества доступных измерений,
и система уравнений оказывается недоопределенной, а значит имеет бесконечно много решений.

В этой ситуации представляется разумным выбрать в качестве решения системы~\ref{gm_pinv}
такое, которое обладает минимальной нормой.
На языке оптимизации такую задачу можно сформулировать как

\begin{equation}
    \norm{\vs}^2 \rightarrow \underset{\vs}{\min},
    s.t.: \mG \vs = \vx
    \label{pinv_opt_criterion}
\end{equation}

Отметим вновь, что в случае обратной задачи МЭЭГ матрица $\mathbf{G}$ является
<<широкой>>, т.е.  количество ее столбцов больше, чем количество строк. Примем
здесь также, что столбцы матрицы $\mathbf{G}$ образуют полную систему векторов,
что как правило выполняется на практике при условии, что количество сенсоров
достаточно мало. В этом случае матрица $\mathbf{GG^T}$ является обратимой.

Выпишем функцию Лагранжа для оптимизационной задачи~\ref{pinv_opt_criterion}:

\begin{equation}
    f_L(\mathbf{s}, \lambda) = \norm{\vs}^2 + \lambda \left(\mathbf{Gs - x}\right)
\end{equation}

Выпишем и приравняем частные производные лагранжиана по каждой из переменных:

\begin{gather}
    \frac{\partial f_L(\mathbf{s}, \lambda)}{\partial \lambda} = \mathbf{Gs - x} = 0
    \label{dL_first} \\
    \frac{\partial f_L(\mathbf{s}, \lambda)}{\partial \mathbf{s}} = 2\mathbf{s}
    + \mathbf{G}^T\mathbf{\lambda} = 0
    \label{dL_second}
\end{gather}

Далее выразим $\mathbf{s}$ из уравнения $\ref{dL_second}$ и подставим
полученное выражение в~\ref{dL_first}:

\begin{gather}
    \mathbf{s} = - \frac{1}{2} \mathbf{G^T}\lambda
    \label{dL_second_s_expressed}\\
    \mathbf{Gs} = - \frac{1}{2} \mathbf{G} \mathbf{G}^T\lambda = \mathbf{x}
    \label{dL_second_to_first}
\end{gather}

Из~\ref{dL_second_to_first} получим выражение для $\lambda$:

\begin{equation}
    \lambda = - 2 {(\mathbf{G}\mathbf{G}^T)}^{-1}\mathbf{x}
    \label{pinv_lambda_expression}
\end{equation}

Наконец, подставив~\ref{pinv_lambda_expression} в~\ref{dL_second_s_expressed}, получим
окончательное выражение для $\mathbf{s}$:

\begin{equation}
    \mathbf{s} = \mathbf{G}^T{(\mathbf{GG}^T)}^{-1}\mathbf{x}
\end{equation}

Как видим, полученное решение имеет структуру псевдообратной матрицы,
полученной в явном виде для матрицы, столбцы которой образуют полную систему
векторов.

Мы показали таким образом, что псевдообратная матрица в обратной задаче МЭЭГ
производит решение, в точности удовлетворяющее исходной линейной системе, и при
этом обладающее минимальной нормой среди бесконечного множества таких решений.

Вместе с тем, несмотря на казалось бы очевидные преимущества такого способа
оценки источников в обратной задаче МЭЭГ, на практике описанный выше метод
никогда не используется. Это вызвано сочетанием двух обстоятельсв. 

Во-первых, реальные данные, как уже неоднократно упоминалось ранее, неизбежно
содержат шумовую компоненту, которую можно моделировать как некоторую добавку
$\omega$ к исходной линейной системе, с помощью которой мы будем оценивать
распределение источников $\mathbf{\hat{s}}$:

\begin{gather}
    \mathbf{G\hat{s}} = \mathbf{x}_{noisy}
    \label{original_linear_w_noise}\\
    \mathbf{x}_{noisy} = \mathbf{x} + \mathbf{\omega}
\end{gather}
Отметим, что на этот раз незашумленные измерения $\mathbf{x}$ представляют
собой некую математическую абстракцию"--- прямого доступа к этой величине мы
не имеем.

Во-вторых, хоть матрица $\mathbf{G}\mathbf{G}^T$ и является обратимой, она как
правило плохо обусловлена в силу того, что топографии (столбцы матрицы
$\mathbf{G}$) сильно коррелируют друг с другом.  Эти два фактора
приводят к тому, что вклад шумовой компоненты $\omega$ в оценку $\mathbf{s}$
значительно усиливается ввиду наличия больших собственных значений в матрице
${(\mathbf{G}\mathbf{G}^T)}^{-1}$:

\begin{gather*}
    \mathbf{\hat{s}} \stackrel{def}{=} \mathbf{G}^T{(\mathbf{G}\mathbf{G}^T)}^{-1}\mathbf{x}_{noisy} = 
    \mathbf{G}^T{(\mathbf{G}\mathbf{G}^T)}^{-1}\mathbf{x} +
    \mathbf{G}^T{(\mathbf{G}\mathbf{G}^T)}^{-1}\omega\\
    \mathbf{s} = \mathbf{G}^T{(\mathbf{G}\mathbf{G}^T)}^{-1}\mathbf{x}
    \\
    \mathbf{\hat{s}} =  \mathbf{s + G^T{(GG^T)}}^{-1}\omega
\end{gather*}

Иными словами, оценка $\mathbf{\hat{s}}$ перестает адекватно отражать реальное распределение
источников $\mathbf{s}$ из-за шумовой добавки $\mathbf{G^T{(GG^T)}}^{-1}\omega$, усиленной
неустойчивым обращением матрицы.

Очевидная модификация метода, направленного на устранение этого недостатка
посредством стабилизации обращения матрицы $\mathbf{GG^T}$ получила название
оценки методом минимальной нормы (Minimum Norm Estimate или сокращенно MNE)
(\cite{MNE_paper}).

\subsection{Оценка методом минимальной нормы.}

Оценка методом минимальной нормы является на сегодняшний день одним из наиболее
популярных алгоритмов анализа распределения источников в электрофизиологии, а
также представляет собой отправную точку для множества других методов, тем или
иным способом улучшающих его работу. К таким методам-модификациям относятся
прежде всего алгоритмы sLORETA и dSPM (\cite{sLORETA_paper, dSPM_paper}).

Суть алгоритма MNE, как уже было сказано, заключается в стабилизации обращения
матрицы $\mathbf{GG^T}$ посредством поднятия ее диагонали. Аналогичная идея
использовалась в уже описанном нами методе DICS (\ref{DICS_subsection}).
Оценка источников методом минимальной нормы таким образом задается соотношением

\begin{equation}
    \mathbf{\hat{s}} = \mathbf{G}^T(\mathbf{GG}^T + \alpha \mathbf{I}_m)^{-1}\mathbf{x}_{noisy},
    \label{MNE_estimation_formula}
\end{equation}

где $\alpha$"--- параметр регуляризации, а $\mathbf{I}_m$"--- единичная матрица
размера $m \times m$, где $m$"--- количество сенсоров. При должном выборе параметра
регуляризации $\alpha$ обращение матрицы $\mathbf{GG^T + \alpha I}_m$
становится теперь численно устойчивым, что ограничивает неконтролируемое
усиление шумовой компоненты, наблюдаемое для оценок, полученных умножением на
псевдообратную матрицу.

Отметим здесь, что формула~\ref{MNE_estimation_formula} может быть переписана в
эквивалентном виде:

\begin{equation}
    \mathbf{\hat{s}} = (\mathbf{G^TG} + \alpha \mathbf{I}_n)^{-1}\mathbf{G}^T\mathbf{x}_{noisy},
    \label{MNE_estimation_formula_v2}
\end{equation}

что легко проверить, подставив сингулярное разложением матрицы $\mathbf{G}$ в
обе формулы~\ref{MNE_estimation_formula}, \ref{MNE_estimation_formula_v2},
упростив полученные выражения и сравнив результаты.

При этом исходная зашумленная система теперь решается не в точности, а
приближенно; параметр $\alpha$ же регулирует баланс между
тем, насколько мала норма восстановленного решения и тем, насколько
хорошо это решение объясняет данные. Количественной оценкой последнего
является норма разности между решением, умноженным на прямую модель,
и вектором измерений.

Более формально, оценка~\ref{MNE_estimation_formula_v2} может быть найдена
как решение следующей задачи оптимизации:

\begin{equation}
    \norm{\mathbf{G\hat{s} - x}_{noisy}}^2 + \alpha\norm{\mathbf{\hat{s}}}^2 \rightarrow \underset{\mathbf{\hat{s}}}{\min}
    \label{MNE_optimization_crit}
\end{equation}

Действительно, дифференцируя функционал~\ref{MNE_optimization_crit} по $\mathbf{s}$,
получим:

\begin{gather*}
    \mathbf{G^T(G\hat{s} - x}_{noisy}) + \alpha\mathbf{\hat{s}} = 0\\
    \mathbf{G^TG\hat{s} + \alpha \hat{s} = G^Tx}_{noisy}\\
    \mathbf{\hat{s} = (G^TG + \alpha I_n)^{-1}G^Tx}_{noisy}
\end{gather*}


Таким образом становится понятен смысл метода минимальной нормы: с одной
стороны, метод пытается как можно точнее решить исходную систему уравнений, в
том числе и объясняя и шум в данных, а с другой он стремится ограничить норму
решения, не давая ему слишком сильно расти в попытках угнаться за объяснением
шумовой компоненты, которая требует значительного роста нормы решения в силу
плохой обусловленности матрицы $\mathbf{GG^T}$. Параметр регуляризации $\alpha$
при этом отвечает за баланс между этими двумя конкурирующими процессами.

Существует однако еще один способ взглянуть на метод минимальной нормы,
позволяющий как по-новому переосмыслить сам метод, так и существенно обобщить
его, сформировав определенный математический каркас, на основе которого
может быть сформулировано множество других методов поиска источников
в обратной задаче МЭЭГ. Этот способ заключается в описании обратной задачи
на языке байесовской вероятности.

Начнем с рассмотрения исходной системы уравнений~\ref{original_linear_w_noise}:

\begin{equation}
    \mathbf{G\hat{s}} = \mathbf{x}_{noisy} = \mathbf{x} + \omega
\end{equation}

При этом мы хотим найти оценку $\mathbf{\hat{s}}$ для величины $\mathbf{s}$,
для которой выполнено

\begin{equation}
    \mathbf{Gs} = \mathbf{x}.
\end{equation}

Предположим, что шум $\omega$ распределен нормально с единичной
матрицей ковариации и нулевым средним:

\begin{equation}
    \mathbf{Gs - x}_{noisy} = -\omega \sim \mathcal{N}(0, \mathbf{I}_m)
\end{equation}

Предположим также, что источники $\mathbf{s}$ некоррелированы и задаются
многомерным нормальным распределением с нулевым средним и дисперсией $\alpha$:

\begin{equation}
    \mathbf{s} \sim \mathcal{N}(0, \alpha\mathbf{I}_n)
\end{equation}

В этом случае критерий максимума апостериорной вероятности говорит нам, что
наиболee вероятным распределением (здесь пространственным, а не вероятностным)
источников $\mathbf{\hat{s}}$ с учетом наблюдаемых измерений
$\mathbf{x}_{noisy}$ будет такое, которое максимизирует апостериорную
плотность вероятности для величины $\mathbf{s}$:

\begin{equation}
    \mathbf{\hat{s}} =
    \underset{\mathbf{s}}{argmax} \left(
        e^{-\norm{\mathbf{Gs - x}_{noisy}}^2} e^{-\alpha\norm{\mathbf{s}}^2}
    \right)
    \label{MAP_crit_for_MNE}
\end{equation}

Логарифмируя соотношение~\ref{MAP_crit_for_MNE}, получим следующую формулу
для оценки $\mathbf{\hat{s}}$:

\begin{equation}
    \mathbf{\hat{s}} =
    \underset{\mathbf{s}}{argmin} \left(
        \norm{\mathbf{Gs - x}_{noisy}}^2 + \alpha\norm{\mathbf{s}}^2
    \right)
    \label{logMAP_crit_for_MNE}
\end{equation}

Как видим, критерий апостериорного максимума в точности привел нас
к задаче оптимизации~\ref{MNE_optimization_crit}, сформулированной для
метода MNE. На этот раз, однако, статистическая формулировка позволяет
нам увидеть метод MNE в перспективе.

Во-первых, становится ясна роль параметра регуляризации $\alpha$:
в вероятностной формулировке этот гиперпараметр вводится как дисперсия
источников при условии, что дисперсия шума на сенсорах равна единице,
то есть представляет собой квадрат отношения сигнал-шум.

Во-вторых, становится понятно, что выбор функционала качества для метода MNE
обусловлен предположениями о распределениях вероятностей шумовой компоненты и
источников. И если предположение о нормальном некоррелированном шуме можно
назвать естественным, то предположение о нормальности и некоррелированности
источников не кажется столь очевидным.  Немедленно возникает вопрос, как
изменятся свойства метода и можно ли его улучшить, если выбирать функцию
распределения априорной вероятности по-другому. В этом и состоит обобщающая
сила вероятностного взгляда на обратную задачу.

В действительности, аргументированный выбор функции распределения априорной
вероятности предполагает наличие определенных знаний о характере распределения
источников в конкретной задаче поиска активации по МЭЭГ измерениям. Источником
таких знаний может служить прежде всего информация об анатомическом строении
мозга испытуемого, а также знания о физиологических процессах, характерных для
рассматриваемой экспериментальной парадигмы.

Вместе с тем, функция распределения априорной вероятности может быть выбрана и из
общих соображений исходя из желаемых свойств решения. Дело в том, что одним из
широко известных свойств решений, полученных при помощи метода MNE, является
сглаживание источников по пространству. Речь идет о том, что в случае,
если истинная активация распределена в пространстве ``фокально'', то есть
по относительно небольшому участку коры, оценка этой активации методом
MNE как правило распределена по области, своим размером сильно превышающей
истинную, затрудняя таким образом оценку мощности источника сигнала в
случаях, когда несколько источников находятся близко друг к другу.
Кроме того сильным пространственным сглаживанием осложнена оценка фазовой
связности источников, восстановленных методом MNE, в силу высокой вероятности
взаимной протечки сигнала.

С другой стороны интуиция подсказывает нам, что в случае поиска источника,
сосредоточенного в одной точке, наиболее простым решением, объясняющим
наблюдения, должен быть единственный токовый диполь, расположенный в этой
точке. Однако метод MNE в силу пространственного сглаживания будет всегда
давать оценку, рассредоточенную пятном вблизи точки, которая действительно была
активна, т.е. оценка будет содержать целую группу активных диполей.
Такое положение веще наводит на мысль, что руководствуясь принципом
максимальной простоты решения, мы должны пересмотреть наш критерий простоты.

При формулировке метода псевдообратной матрицы, а также метода MNE мы
отталкивались от минимизации $L2$-нормы решения. Очевидным решением описанной
выше проблемы является модификация критерия: мы должны минимизировать
количество ненулевых компонент в векторе источников $\mathbf{s}$. Такой
критерий формализуется как минимизация $L_0$-"нормы" решения.  (Кавычки
необходимы как напоминание, что в действительности $L_0$ не является нормой,
т.к. для нее нарушается свойство однородности.) Так как решение в то же время
должно минимизировать норму невязки $\norm{\mathbf{Gs - x}}_{L_2}$, функционал
качества решения по аналогии с~\ref{MNE_optimization_crit} запишется в виде

\begin{equation}
    \norm{\mathbf{G\hat{s} - x}_{noisy}}^2_{L_2} + \alpha\norm{\mathbf{\hat{s}}}_{L_0}
    \rightarrow \underset{\mathbf{\hat{s}}}{\min}.
    \label{l0_optimization_crit}
\end{equation}

В вероятностной постановке такой оптимизационный критерий соответствует
выбору в качестве априорной функции распределения многомерного распределения
Бернулли в предположении, что все источники независимы. Иными словами,
для каждого источника предполагается, что он активен с вероятностью
$p$ и не активен с вероятностью $1-p$, а вероятность совместной активации
любого набора диполей-источников равна произведению вероятностей активации
каждого из них в отдельности.
Для отдельно взятого источника с индексом
$i$ такая функция распределения будет выглядеть как

\begin{equation}
    P_{s_i} = p^{(1 - \delta(s_i))} (1-p)^{\delta(s_i)}
\end{equation}

где $\delta(s_i)$ представляет собой дельта-функцию. Эта формула представляет
собой обобщение функции плотности вероятности для одного испытания Бернулли
на случай непрерывной переменной.

Для совокупной активации с учетом независимости активаций отдельных диполей
(предполагается, что активации всех диполей равновероятны с вероятностью $p$)
функция распределения будет выглядеть как произведение функций распределения
для компонент вектора $\mathbf{s}$. Выпишем ее и приведем к форме функции
распределения из экспоненциального семейства, чтобы ее было удобнее использовать
в критерии максимума апостериори:

\begin{multline}
    P_{\mathbf{s}} = \prod_{i=1}^{N} p^{(1-\delta(s_i))} (1-p)^{\delta(s_i)}=
    \exp\left(\sum_{i=1}^N \left(\log(p^{1-\delta(s_i)}) + \log((1-p)^{\delta(s_i)})\right)\right)=\\
    =\exp\left(\sum_{i=1}^N \left(\log(p) - \delta(s_i) \log(p) + \delta(s_i)\log(1-p)\right)\right)=\\
    =\exp\left(-\sum_{i=1}^N \left(\delta(s_i) \log\left(\frac{p}{1-p}\right) - \log(p)\right)\right) = \\
    =\exp\left(-\log\left(\frac{p}{1-p}\right) \norm{\mathbf{s}}_{L_0} + N\log(p)\right)\right)
    \label{l0_multivariate_bernoulli_distribution}
\end{multline}

Записывая для априорного распределения
~\ref{l0_multivariate_bernoulli_distribution} критерий максимума апостериори,
логарифмируя и учитывая, что $N\log(p)$ не зависит от $\mathbf{s}$, получим
оптимизационный критерий~\ref{l0_optimization_crit}. Отметим, что вероятностная
формулировка позволяет в явном виде получить выражение для параметра $\alpha$
через вероятность активации одного источника:

\begin{equation}
    \alpha = \log\left(\frac{p}{1-p}\right)
\end{equation}


Проблема с задачей оптимизации~\ref{l0_optimization_crit} заключается в том,
что она является NP-сложной, а значит на практике метод, основанный на
оптимизации функционала~\ref{l0_optimization_crit} использовать не получится.

Тем не менее, существуют подходы к решению обратной задачи,
основанные на аппроксимации $L_0$-нормы таким образом, чтобы
приближенная задача оптимизации стала алгоритмически разрешимой.

Хронологически первым методом решения обратной задачи для МЭЭГ,
основанным на этой идее стал метод минимальных токов
(Minimum Current Estimate, MCE).

\subsubsection{Метод минимальных токов (MCE)}

В своей основе суть метода заключается в аппроксимации дельта-функции,
используемой в "норме" $L_0$ из функционала~\ref{l0_optimization_crit} модулем,
соответствующим норме $L_1$ и решении возникающей задачи оптимизации. Выбор
нормы $L_1$ при формулировке метода через критерий максимума апостериори
соответствует выбору в качестве априорной функции распределения источников
распределения Лапласа.

Метод MCE в силу специфического выбора функции распределения источников
позволяет получать спарсные решения, т.е. решения, сильно разреженные по
пространству, демонстрируя свойства аналогичного ему метода
LASSO~\ref{lasso_paper}, используемого в теории оценивания и машинном обучении.
Как и метод LASSO, метод MCE стремится обнулить максимальное количество
компонент вектора $\mathbf{s}$, чего и следует ожидать от метода, основанного
на аппроксимации нормы $L_0$.

При этом метод минимальных токов наряду с выгодным для определенных приложений
свойством спарсности обладает также рядом недостатков.

Во-первых, метод является более требовательным к вычислениям по сравнению с
MNE, так как формула оценки методом MCE не может быть выписана в явном виде и
решение необходимо получать численно. Кроме того, оптимизируемый функционал
качества больше не является гладкой функцией, что затрудняет построение
эффективной численной схемы.  Иными словами, расплатой за свойство
пространственной разреженности решения являются сложности с поиском экстремума
функционала качества.

Во-вторых, метод никак не учитывает временную динамику сигнала, так как поиск
решения происходит для каждого временного среза отдельно.  На практике это
приводит к тому, что временные ряды, восстановленные для источников методом
MCE, не являются гладкими: для соседних временных срезов могут быть обнулены
абсолютно разные компоненты вектора $\mathbf{s}$. Здесь стоит отметить, что
хотя метод MNE также не учитывает временную структуру сигнала явно, он, тем не
менее, сохраняет гладкость сигнала по времени, так как представляет собой
лишь умножение сигнала на невырожденную матрицу.

Чтобы убедиться в том, что метод MCE нарушает временную гладкость сигнала,
рассмотрим, как будет выглядеть его функционал качества для каждого среза по
времени $t$:

\begin{equation}
    \norm{\mathbf{G\hat{s}}(t) - \mathbf{x}_{noisy}(t)}^2_{L_2} + \alpha\sum_i\abs{\hat{s_i}(t)}
    \rightarrow \underset{\mathbf{\hat{s}}}{\min}.
    \label{l1_optimization_crit}
\end{equation}

Несложно убедиться, что для совокупности всех временных срезов, для которых
решается обратная задача (их количество обозначим $T$), эквивалентный
функционал качества будет выглядеть как

\begin{equation}
    \norm{\mathbf{G\hat{S}} - \mathbf{X}_{noisy}}^2_{L_2} + \alpha\sum_{i,j}\abs{\hat{S}_{i,j}}
    \rightarrow \underset{\mathbf{\hat{s}}}{\min},
    \label{l1_optimization_crit_matrix}
\end{equation}
где $\mathbf{\hat{S}}$ --- матрица размером $N\times T$, содержащая по строкам временные ряды,
оцененные для каждого источника
