\chapter{Решение обратной задачи в пространстве матриц кросс-спектральной плотности}~\label{chapt2}
% План.
% Методы решения обратной задачи для МЭГ/ЭЭГ.
% Сканирующий подход. Нахождение ориентации диполя.
% MUSIC, RAP-MUSIC, бимформеры. Сканирующие методы решения обратной задачи для анализа коннективностей.
% Алгоритм DICS. Модификация DICS для оценки imcoh в пр-ве источников. Wedge MUSIC.
% Методы глолбальной оптимизации.
% Оценка активностей при помощи псевдообратной матрицы.
% Априорная информация о структуре решения. MNE, MinimumCurrent, mxNormMNE.
% Адаптация MxNormMNE к решению обратной задачи для поиска синхронностей.
% Техники ускорения расчета и оптимизации вычислительных ресурсов.
% Active set и связь с MUSIC.
% Критерий останова алгоритма.
% Сокращение размерности пр-ва сенсоров.

В предыдущей главе нами была получена система уравнений, связывающая
коэффициенты матрицы кросс-спектральной плотности мощности в пространстве
источников с аналогичными коэффициентами в пространстве сенсоров. Хотя это
уравнение и является линейными относительно неизвестных величин $c_{ij}^{ss}$,
процедура нахождения решения, которое адекватно описывало бы поведение реальных
систем, осложнено тем фактом, что рассматриваемая система уравнений существенно
недоопределена, а следовательно для нее существует бесконечное количество
решений, далеко не все из которых осуществимы для реальных систем
взаимодействующих нейронных ансамблей.

Вообще, система уравнений~\ref{eq:cp_final_re_im} для фиксированных ориентаций
диполей по своей структуре ничем кроме шумового слагаемого не отличается от
систем~\ref{eq:BV_generative_matrix}.
Система уравнений~\ref{eq:BV_generative_matrix} на неизвестные величины $\mathcal{Q}$ также
является линейной и недоопределенной. Некоторые отличия возникают лишь при
рассмотрении свободно ориентированных диполей.

Задачу оценки величин, положений и ориентаций токовых диполей $\mathcal{Q}$ на
основании измерений $\mathcal{B}, \mathcal{V}$ в электрофизиологии принятно
называть \emph{обратной задачей} МЭГ/ЭЭГ.  Как и в случае оценки
кросс-спектральных коэффициентов, решение обратной задачи МЭГ/ЭЭГ не
единственно.  Для выбора какого-либо одного решения $\mathcal{Q}$ по коре
применяют различные эвристики, ограничивающие выбор из бесконечного множества
возможных вариантов.  Как правило, получающееся решение отвечает тому или иному
критерию оптимальности в соответствии с используемой эвристикой, при условии,
что предположения модели выполняются.  Во введении к этой главе мы рассмотрим
основные методы решения обратной задачи для поиска активных токовых диполей на
коре, а затем перейдем к рассмотрению методик для оценки кросс-спектральных
коэффициентов с учетом свободной ориентации.

Все существующие методики решения обратной задачи МЭГ/ЭЭГ можно условно
разделить на два класса.
К первому классу относятся алгоритмы, основанные на
поиске заранее заданного числа эквивалентных токовых диполей, объясняющих измерения
наилучшим образом. При этом оцениваются положения и ориентации этих диполей.
К этому классу относится необходимый нам для дальнейшего изложения алгоритм % Так ли это?
MUSIC (multiple signal classification), а также его модификация RAP-MUSIC
(recursively applied and projected MUSIC), которые условно можно назвать сканирующими.

Второй класс алгоритмов, называемых в литературе <<имиджинговыми>>, ставит задачу
отыскания активности, распределенной сразу по всей коре. Ко второй группе методов
можно условно отнести методы оптимальной пространственной фильтрации, которые
восстанавливают сигнал отдельно для каждой выбранной точки коры в соответствии
с неким локальным критерием оптимальности, а также методы, основанные на выборе
решения с минимальной нормой.

Суть подхода оптимальной фильтрации состоит в том, что для
фиксированной точки внутри объема мозга ставится задача нахождения
пространственного фильтра, оптимизирующего определенную характеристику сигнала,
восстанавливаемого при помощи этого фильтра. В качестве такой характеристики
может выступать, например, отношение сигнал/шум, или же мы можем
руководствоваться критерием минимизации протечки сигнала от других источников в
точку, в которой мы хотим восстановить активность.  Здесь важно отметить, что
конкретный вид решения, полученного в результате оптимизации выбранного
функционала качества будет зависить также от предполагаемой пространственной
структуры шума.

Отметим, что структура восстановленной после применения совокупности найденных
фильтров активации на коре при таком подходе, вообще говоря, субоптимальна с
точки зрения объяснения сигнала, измеренного сенсорами (так как мы
оптимизировали другой функционал качества). Проблема недоопределенности
системы уравнений при этом в некотором смысле остается за скобками, так как для
каждой точки коры решение восстанавливается индивидуально --- без учета вклада
в решение активаций, восстановленных в других точках коры.  Таким образом, для
алгоритмов оптимальной фильтрации найденное решение является
оптимальным в локальном, но не в глобальном смысле.

Задача отыскания активаций, наилучшим образом объясняющих измерения, (т.е.
оптимальных в глобальном смысле) ставится для другого подкласса имиджинговых
методов.
При этом, как уже было отмечено выше, в силу
недоопределенности системы линейных уравнений, связывающих активации на коре с
сигналом на сенсорах, существует бесконечное множество конфигураций источников,
идеально объясняющих померенный сигнал. Тем не менее, среди таких решений в
силу зашумленности измерений а также неточностей при построении прямой модели
реальное распределение активаций (в выделенных точках) коры не содержится, так
как эти <<идеальные>> с точки зрения объяснения измерений решения объясняют в том
числе и шумовую компоненту, которая зачастую оказывается больше или сравнима по
амплитуде с истинной активацией.

Итак, при решении обратной задачи методами глобальной оптимизации существует
две проблемы: бесконечное множество возможных решений и зашумленность
измерений. Чтобы справиться с первой проблемой, для выбора из бесконечного
множества решений пользуются критерием минимальности нормы решения. Иными
словами, среди всех возможных конфигураций первичных токов в объеме (или на
поверхности коры) мозга в качестве решения выбирается такая конфигурация, норма
которой минимальна среди всех возможных.  Условие минимальности нормы в
некотором смысле является следствие принципа бритвы Оккама: мы ищем наиболее
простое решение, удовлетворяющее наблюдениям. Какие решения при этом считать
простыми"--- неочевидный вопрос.  Ответ на него зависит от выбора конкретного
вида нормы, которую мы хотим минимизировать.  Наиболее популярными вариантами
являются $L_2$- и $L_1$-нормы.  Наиболее простой пример минимальной $L_2$-нормы
решения соответствует случаю, когда проблему зашумленности данных мы оставляем
без внимания.  Тогда решение, соответствующее минимуму $L_2$-нормы, получается
применением оператора, соответствующего псевдообратной матрице, взятой для
матрицы прямой модели.  Для $L_1$-нормы ситуация несколько сложнее, так как
решение не может быть получено в явном виде, и требуется численная оптимизация
соответствующего функционала, сводящаяся к задаче линейного программирования.

Рассмотрим теперь, каким образом решается проблема зашумленности данных. Здесь
вновь существует два подхода.  Первый из них используется значительно реже и
состоит в удалении шумовой компоненты из данных посредством сокращенного
сингулярного разложения матрицы прямой модели. Такой подход, например,
использовали авторы, метода Minimum Current Estimate (MCE)~\ref{mce},
порождающего решения с минимальной $L_1$-нормой.

Другой, более популярный подход состоит в использовании тихоновской
регуляризации. В рамках этого подхода норма решения и глобальная ошибка в
объяснении измерений минимизируются совместно, как части одного общего
функционала качества, позволяя тем самым соблюсти баланс между простотой
решения и тем, насколько хорошо оно объясняет измерения, (в том числе,
содержащийся в них шум). Соотношение между <<простотой>> решения и величиной ошибки при
таком подходе можно регулировать настраивая величину метапараметра,
называемого параметром регуляризации. Меняя значение параметра регуляризации
мы стремимся найти такое значение, при котором полученное решение объясняет
только <<полезную>> часть сигнала, записанного сенсорами и полностью игнорирует
шумовую компоненту.

Отметим, что с точки зрения байесовской статистики тихоновская регуляризация
эквивалентна нахождению такого решения обратной задачи, которое соответствует
точке максимума апостериорной плотности вероятности. Минимизируемая норма решения в такой
интерпретации задается априорной плотностью распределения вероятности.

Среди методов, основанных на тихоновской регуляризации, отметим прежде всего
Minimum Norm Estimate (MNE) \ref{mne}, минимизирующий $L_2$-норму решения,
и его вариацию"--- метод dSPM, нормирующий величину восстановленного значения первичного
тока в каждой точке на оцененную величину шума в ней же.

\section{Сканирующие алгоритмы}~\label{sec:scan_algorithms}

Наиболее естественным алгоритма поиска фиксированного числа эквивалентных
токовых диполей (dipole fitting), объясняющих данные наилучшим способом,
является оптимизация их положений и ориентаций методом наименьших квадратов.
Такой подход, однако, обладает существенными недостатками, к которым относится
невыпуклость целевой функции при такой оптимизации,
что приводит к застряванию алгоритма в локальных минимумах.

\subsection{MUSIC}
Чтобы обойти эту проблему, Мошер и Лихи предложили использовать для
поиска активных токовых диполей алгоритм MUSIC~\ref{MUSIC, Schmidt}, разработанный
и использовавшийся ранее в радиопеленгации и сонарах.

Рассмотрим подробно суть метода в применении к данным ЭЭГ/МЭГ.
Начнем с рассмотрения порождающей модели сигнала на сенсорах, как мы это
уже делали для оценки фазовой синхронности (см.~\ref{gm_ts}).
На этот раз, однако, заложим в модель возможность свободной ориентации диполей.
От каждого активного токового диполя на коре будем иметь вклад на сенсорах вида:

\begin{equation}
    \mathbf{x}_k(t) =
        \begin{bmatrix}
            |                 & |              & |              \\
            \mathbf{g}_k^1    & \mathbf{g}_k^2 & \mathbf{g}_k^3 \\
            |                 & |              & |
        \end{bmatrix}
        \left(\begin{array}{ccc}
                s_{k,1}(t)\\
                s_{k,2}(t)\\
                s_{k,3}(t)
            \end{array}
        \right)
        % \mathbf{s}_{\xi}(t),
\end{equation},
где $k$"--- индекс токового диполя $s_{k,i}$"--- компоненты соответствующего
дипольного момента,
$\mathbf{g}_k^i$"--- вектора-топографии $k$-го токового диполя
для трех ориентаций тока ($i=1,2,3$).
Тогда вклад от всех активных токовых диполей будет виден на сенсорах как

\begin{equation}
    \mathbf{x}(t) = \mathbf{G} \mathbf{s}(t) + \mathbf{\omega}(t),
    \label{gm_music}
\end{equation}

Здесь вновь $\mathbf{s}(t)$ --- $3n$-мерный вектор-столбец активаций источников,
$\mathbf{x}(t)$ --- $m$-мерный вектор-столбец сигналов на сенсорах,
$t$ --- время, а $\mathbf{G}$ --- $m \times 3n$ матрица линейного
отображения пространства источников пространство сенсоров.

Уравнение \ref{gm_music} задает соответствие между пространством источников и пространством
сенсоров для каждого временного среза $t$.
При условии, что было записано $T$ таких срезов,
можем переписать уравнение~\ref{gm_music} в матричной форме:

\begin{equation}
    \mathbf{X} = \mathbf{G} \mathbf{S} + \mathbf{\Omega}
    \label{gm_music_matrix}
\end{equation}

Заметим, что столбцы матрицы $\mathbf{X}$ порождаются линейными комбинациями
векторов-топографий активных токовых диполей, а значит все возможные конфигурации
наблюдаемого сигнала живут внутри некоторого линейного подпространства линейной оболочки этих
топографий. Это линейное подпространство называется \emph{подпространством сигнала}.
При этом количество активных токовых диполей $r$ задает размерность этого подпространства
Чтобы выделить его, применим к матрице $\mathbf{X}$
сингулярное разложение и зафиксируем первые $r$ левых собственных векторов:

\begin{gather}
    \mathbf{U}, \mathbf{S}, \mathbf{V} = svd(\mathbf{X}) \\
    \mathbf{U} =
        \begin{bmatrix}
            |                 &               & |            \\
            \mathbf{u}_1      & \cdots        & \mathbf{u}_m \\
            |                 &               & |
        \end{bmatrix}\\
    \mathbf{U}_r = 
        \begin{bmatrix}
            |                 &               & |            \\
            \mathbf{u}_1      & \cdots        & \mathbf{u}_r \\
            |                 &               & |
        \end{bmatrix}
    % \mathbf{U}_m = U[:,:m]
\end{gather}

Матрица $\mathbf{U}_r$ называется \emph{матрицей подпространства сигнала};
ее столбцы задают ортонормальный базис этого подпространства.
Чтобы найти все активные токовые диполи, для каждого источника на коре
вычислают \emph{корреляцию подпространств} между подпространством сигнала и
линейной оболочкой трех (двух в случае МЭГ) топографий, соответствующих данному источнику.
Корреляция подпространств $C_k$ вычисляется как максимальное собственное число
произведения матрицы левых собственных векторов для топографий $k$-го источника
и матрицы подпространства сигнала:

\begin{gather}
    \mathbf{U}_{k,g}, \mathbf{S}_{k,g}, \mathbf{V}_{k,g} = svd\left(
            \begin{bmatrix}
                |                 & |              & |              \\
                \mathbf{g}_k^1    & \mathbf{g}_k^2 & \mathbf{g}_k^3 \\
                |                 & |              & |
            \end{bmatrix}
     \right)\\
     C_k = \lambda_{max}(\mathbf{U}_{k,g} \mathbf{U}_r^T)
\end{gather}

Активными считаются такие токовые диполи, для которых величина $C_k$ выше некоторого
заранее порога (авторы статьи \ref{MUSIC} рекомендуют для порога значение 0.95).

Таким образом, для нахождения всех активных токовых диполей необходимо
<<просканировать>> объем или поверхность мозга на предмет источников, для которых
корреляция подпространств с подпространством сигнала превышает некоторое
пороговое значение. 

Одним из сущетственных недостатков подхода MUSIC является потенциальная
сложность в разделении нескольких одновременно активных токовых диполей, а также
в выделении ситуации, когда реальный источник имел не фокальную, а распределенную
структуру. В этом случае в распределении в объеме или по поверхности коры
величин $C_k$ будут присутствовать локальные максимумы, анализ которых требует дополнительных
усилий.


\subsection{RAP-MUSIC}~\label{subsection:rap_music}

Чтобы справиться с этой проблемой, Мошер и Лихи предложили обобщение метода MUSIC, которое
они назвали Recursively applied and projected MUSIC или RAP-MUSIC.

Идея метода заключается в последовательном применении MUSIC-скана для нахождения
диполя с максимальным значением $С$ с последующей проекцией матрицы данных и топографий оставшихся источников
ортогонально подпространству топографий диполя с максимальным $C_k$, с пердыдущей итерации.

Процедура продолжается до тех пор, пока максимальное по источникам значение корреляции подпространств
на новом шаге не опустится ниже порогового значения.

Описание алгоритма RAP-MUSIC на языке псевдокода приведено в листинге \ref{rap_music_listing}.

\begin{ListingEnv}[!h]
    \begin{lstlisting}[language=Python]
def RAP_MUSIC(X, G, threshold):
    """
    Параметры
    ---------
    X : матрица измерений
    G : матрица прямой модели для свободной ориентации
    threshold : порог корреляции подпространств

    Возвращает
    ----------
    active_dipole_indices : индексы найденных диполей

    """
    # инициализируем пустой список индексов активных диполей
    active_dipole_indices = []

    while True:
        # ищем корреляции подпространств для каждого источника
        C = MUSIC_scan(X, G)
        if max(C) < threshold:
            break
        k = argmax(C)
        active_dipole_indices.append(k) 
        # проецируем от диполей k-го источника
        X, G = project_away_from_k(X, G, k) 

    return active_dipole_indices
    \end{lstlisting}
    \label{rap_music_listing}
\end{ListingEnv}

Такая процедура позволяет исключить вклад в измерения от уже найденных
диполей, последовательно объясняя оставшуюся в данных дисперсию.
Пороговый критерий останова алгоритма гарантирует отсутствие ложноположительных
срабатываний.

Отдельного упоминания для алгоритмов MUSIC и RAP-MUSIC заслуживает выбор
ранга подпространства сигнала. С одной стороны, этот ранг должен совпадать
с количеством реально присутствующих в данных диполей. С другой стороны,
a priori, до применения алгоритма это число не известно. Хорошая новость заключается
в том, что переоценка ранга подпространства сигнала практически не влияет на
качество работы этзих двух алгоритмов,
поэтому авторы статей~\ref{MUSIC, RAP_MUSIC} в качестве общей рекомендации
советуют выбирать значения этого ранга больше реально ожидаемых.

Оба алгоритма, будучи достаточно простыми и вычислительно эффективными,
являются удобным инструментом для решения обратной задачи в первом приближении,
что может быть полезно во-первых для примерного понимания картины распределения
источников, а во-вторых в качестве этапа предобработки при решении ОЗ
для сужения количества вариантов поиска с последующим анализом более
изощренными, но не столь вычислительно быстрыми алгоритмами.

\section{Алгоритмы пространственной фильтрации}

Другой класс алгоритмов решения обратной задачи основан на методах оптимальной пространственной
фильтрации сигнала. Различные критерии оптимальности порождают при этом различные алгоритмы.
В общем виде задача построения оптимальных пространственных фильтров для $k$-го источника
сводится к нахождению
матрицы $\mathbf{A}_k$, при умножении которой на матрицу данных $\mathbf{X}$, получается
матрица $\mathbf{S}_k$ размером $3\times T$ временных рядов для трех ортогональных
направлений тока в точке $k$, отвечающая некоторому критерию оптимальности:

\begin{equation}
    f(\mathbf{A}_k \mathbf{X}) \rightarrow opt
\end{equation}

Проведем обзор существующих критериев оптимальности и получающихся из них методов на
основании статьи~\ref{Gross_1999}.

\subsection{Пространственный фильтр, оптимизирующий отношение сигнал-шум в заданной точке}\label{sec:max_snr_filter}

Начнем рассмотрение семейства методов пространственной фильтрации с метода, максимизирующего
отношение сигнал-шум в заданной точке.

Сначала определим отношение сигнал-шум в терминах матриц ковариаций шума и данных.
Пусть $\mathbf{R}_{X,k} = \mathbf{X}\mathbf{X}^T$"--- матрица ковариации сигнала на сенсорах,
приходящего от $k$-го источника
(предполагается что мы уже вычли среднее из временных рядов в матрице $\mathbf{X}$),
а $\mathbf{R}_n$"--- матрица ковариации шума на сенсорах. Тогда отношение
сигнал-шум на сенсорах определяеся как

\begin{equation}
    \rho = \frac{tr(\mathbf{R}_{X,k})}{tr(\mathbf{R}_n)}
\end{equation}

Тогда для отфильтрованного  при помощи фильтра $\mathbf{A}_k$
(матрица размером $m \times 3$) $k$-го источника ОСШ будет выглядеть как

\begin{equation}
    \rho_k = \frac{tr(\mathbf{A}_k \mathbf{R}_{X,k} \mathbf{A}_k^T)}{tr(\mathbf{A}_k \mathbf{R}_n \mathbf{A}_k^T)}
\end{equation}

Тогда задача построения пространственного фильтра $\mathbf{A}_k$,
который максимизирует отношение сигнал-шум
для $k$-го отфильтрованного источника формализуется как

\begin{gather}
    \mathbf{A}_k = \underset{\mathbf{A}_k}{argmax}(\rho_k) =
    \underset{\mathbf{A}_k}{argmax}\left(
        \frac{tr(\mathbf{A}_k \mathbf{R}_{X,k} \mathbf{A}_k^T)}
             {tr(\mathbf{A}_k \mathbf{R}_n \mathbf{A}_k^T)}
         \right)\\
    \label{max_snr_objective}
    s.t.: \mathbf{A}_k\mathbf{A}_k^T = \mathbf{I}_3
\end{gather}

Ограничение необходимо, чтобы фильтр не менял пространственную структуру шума.
Шум будем предполагать пространственно белым с дисперсией $\sigma^2_n$
В общем случае это предположение можно удовлетворить
предварительным отбеливанием данных, применив к ним
оператор $\mathbf{R}_n^{-\frac{1}{2}}$.
% Так как к шумовым источникам при восстановлении активности в
% $k$-ой точке коры относится также любая мозговая активность,
% приходящая из других точек коры, на практике построение опператора
% $\mathbf{R}^{-\frac{1}{2}}$ требует априорного знания о распределении
% источников по коре, и следовательно невозможно. Мы можем, однако,
% использовать определенные приближения для матрицы отбеливания.
% Например, в качестве матрицы ковариации шума зачастую используется
% матрица $\mathbf{G}\mathbf{G}^T$, которая действительно являлась
% бы матрицей ковариации шума, если бы в каждой точке коры присутствовал
% единичный шумовой источник.


В одномерном случае, соответствующем фиксированной ориентации диполя,
матрица фильтров $\mathbf{A}_k$ вырождается в вектор-строку,
а матрица ковариации для источника с индексом $k$ приобретает вид

\begin{equation}
    \mathbf{R}_{X,k} = \sigma^2 \mathbf{g}_k \mathbf{g}_k^T
\end{equation}

где $\sigma^2_k$ --- дисперсия сигнала.
Для матрицы ковариации шума в предположении, что он является пространственно белым, 
будем иметь $\mathbf{R}_n = \mathbf{I}$.
Тогда~\ref{max_snr_objective} в одномерном случае преобразуется к виду

\begin{equation}
    \mathbf{A}_k =
        \underset{\norm{\mathbf{A}_k} = 1}{argmax}\left(
            \frac{\sigma_k^2\norm{\mathbf{A}_k \mathbf{g}_k}^2}
                 {\sigma_n^2\mathbf{A}_k \mathbf{I} \mathbf{A}_k^T}
             \right) = \frac{\sigma_k^2}{\sigma_n^2}\underset{\norm{\mathbf{A}_k} = 1}{argmax}\left(
                \norm{\mathbf{A}_k \mathbf{g}_k}^2
             \right)
\end{equation}


Очевидно, что среди всех векторов единичной длины максимальное значение ОСШ
будет достигаться на векторе, сонаправленном с ориентацией вектора топографии $\mathbf{g}_k$:

\begin{equation}
    \mathbf{A}_k = \frac{\mathbf{g}_k^T}{\norm{\mathbf{g}_k}}
\end{equation}

В случае свободной ориентации диполя матрица $\mathbf{R}_{X,k}$ примет вид

\begin{equation}
    \mathbf{R}_{X,k} = \sigma_k^2 \left[
            \mathbf{g}_k^1, \mathbf{g}_k^2, \mathbf{g}_k^3 \right]
        {\left[\mathbf{g}_k^1, \mathbf{g}_k^2, \mathbf{g}_k^3\right]}^T
\end{equation}

В этом случае любые три вектора (два в случае МЭГ),
образующие ортонормированный базис подпространства, натянутого на
вектора $\mathbf{g}_k^1, \mathbf{g}_k^2, \mathbf{g}_k^3$,
могут быть использованы как компоненты оптимального фильтра.
В частности, для построения фильтра $\mathbf{A}_k$ можем использовать
первые три левых сингулярных вектора матрицы $\mathbf{g}_k$:

\begin{gather}
    \mathbf{U}_{k,g}, \mathbf{S}_{k,g}, \mathbf{V}_{k,g} = svd\left(
            \begin{bmatrix}
                |                 & |              & |              \\
                \mathbf{g}_k^1    & \mathbf{g}_k^2 & \mathbf{g}_k^3 \\
                |                 & |              & |
            \end{bmatrix}
     \right)\\
    \mathbf{A}_k = 
            \begin{bmatrix}
                |                 & |              & |              \\
                \mathbf{U}_{k, g}^1    & \mathbf{U}_{k,g}^2 & \mathbf{U}_{k,g}^3 \\
                |                 & |              & |
            \end{bmatrix}
\end{gather}

Действительно, проекция векторов фильтра
на ортогональное дополнение линейной оболочки векторов
$\mathbf{g}_k^1, \mathbf{g}_k^2, \mathbf{g}_k^3$,
при подсчете произведения $\mathbf{A}_k \mathbf{g}_k \mathbf{g}_k^T \mathbf{A}_k^T$
будет давать нулевой вклад, а значит, так как вектора $\mathbf{A}_k$
фиксированной длины (единичные), они должны целиком принадлежать
линейной оболочке $span(\mathbf{g}_k^1, \mathbf{g}_k^2, \mathbf{g}_k^3)$.
Вместе с тем, из ограничения $\mathbf{A}_k \mathbf{A}_k^T = \mathbf{I}_3$ следует, что
матрица $\mathbf{A}_k$ является матрицей перехода от одного ортонормированного базиса
линейной оболочки $span(\mathbf{g}_k^1, \mathbf{g}_k^2, \mathbf{g}_k^3)$ к другому,
а произведение $\mathbf{A}_k \mathbf{g}_k \mathbf{g}_k^T \mathbf{A}_k^T$
реализует этот переход.
Но след матрицы является инвариантом при переходе от одного ортонормированного базиса к другому,
а значит $tr(\mathbf{A}_k \mathbf{g}_k \mathbf{g}_k^T \mathbf{A}_k^T)$
будет одинаков для любого $\mathbf{A}_k$, строки которого образуют ортонормированный базис
$span(\mathbf{g}_k^1, \mathbf{g}_k^2, \mathbf{g}_k^3)$.

% Установим теперь связь фильтрации, максимизирующей ОСШ, с алгоритмом MUSIC.
Рассмотрим теперь способ получения фильтра, оценивающего истинную ориентацию диполя
и восстанавливающего сигнал вдоль этой ориентации.
После применения полученной матрицы фильтра $\mathbf{A}_k$ к данным,
мы будем получать три временных ряда, соответствующих трем ортогональным направлениям
тока в точке $k$. Для получения фильтра, восстанавливающего единственный временной ряд
вдоль истинной ориентации тока в $k$-ой точке также пользуются критерием максимизации ОСШ.
В этом случае ставится задача отыскания такой ориентации фильтров
$\boldsymbol{\theta}_k = {(\theta^1, \theta^2, \theta^3)}^T$, для
которой отфильтрованный сигнал $\mathbf{S}_k = \boldsymbol{\theta}_k^T \mathbf{A}_k \mathbf{X}$
будет обладать максимальной мощностью среди всех возможных ориентаций.

Для нахождения такой ориентации достаточно взять собственный вектор матрицы
$\mathbf{A}_k \mathbf{X}\mathbf{X}^T \mathbf{A}_k^T$,
соответствующий ее максимальному собственному числу.
Тогда ориентированный фильтр будет выглядеть как

\begin{equation}
    \mathbf{A}_{k}^{or} = \boldsymbol{\theta}^T \mathbf{A}_k
\end{equation}

Заметим, что процедура построения такого фильтра очень похожа на способ
вычисления корреляции подпространств в алгоритме MUSIC\@.
% Полученный вектор в силу свойств сингулярного разложения
% будет иметь максимальную проекцию на подпространство, натянутое на столбцы
% матрицы $\mathbf{X}$, a значит, так как его длина фиксирована и равна единице,
% этот вектор будет составлять минимальный угол с линейной оболочкой
% столбцов $\mathbf{X}$.

\subsection{Пространственный фильтр, минимизирующий вклад третих источников в оценку}
\label{sec:min_interference_filter}

Рассмотрим теперь пространственный фильтр,
оптимизационным критерием которого является задача минимизации
вклада активных источников, находящихся в отличных от целевой точках коры.
Этот критерий эквивалентен задаче отыскания фильтра, для котрого отфильтрованный
сигнал будет обладать минимальной энергией при условии, что
сигнал из целевой точки фильтром не искажается.

Математически эта задача в случае фиксированной ориентации диполей запишется как

\begin{equation}
    \Expect{\norm{\mathbf{a}_k \mathbf{X}}^2}
    \rightarrow \underset{\mathbf{a}_k}{\min},
    s.t.: \mathbf{a}_k \mathbf{g}_k = 1
    \label{lcmv_objective_nonreg}
\end{equation}

Выпишем соответствующий лагранжиан c учетом того,
что $\mathbf{R}_{\mathbf{X}} = \Expect{\mathbf{X}\mathbf{X}^T}$~---
это матрица ковариации сигнала на сенсорах:

\begin{equation}
    f_L(\mathbf{a}_k, \alpha) = \mathbf{a}_k \mathbf{R}_{\mathbf{X}} \mathbf{a}_k^T +
                                 \alpha (\mathbf{a}_k \mathbf{g}_k - 1)
\end{equation}


Далее, приравняв производную лагранжиана по $\mathbf{a}_k ^ T$ к нулю, будем иметь:

\begin{equation}
    \frac{\partial f_L}{\partial \mathbf{a}_k} = 
    2 \mathbf{R}_{\mathbf{X}} \mathbf{a}_k^T + \alpha \mathbf{g}_k = 0
\end{equation}

Откуда

\begin{equation}
    \mathbf{a}_k = - \frac{\alpha}{2} \mathbf{g}_k^T\mathbf{R}_{\mathbf{X}}^{-1}
\end{equation}

Используя ограничение, получим

\begin{equation}
    1 = \mathbf{a}_k \mathbf{g}_k =
    - \frac{\alpha}{2} \mathbf{g}_k^T\mathbf{R}_{\mathbf{X}}^{-1} \mathbf{g}_k \implies
    \alpha = -\frac{2}{\mathbf{g}_k^T \mathbf{R}_{\mathbf{X}}^{-1} \mathbf{g}_k}
\end{equation}

Окончательно, будем иметь

\begin{equation}
    \mathbf{a}_k =
    \frac{\mathbf{g}_k^T \mathbf{R}_{\mathbf{X}}^{-1}}{\mathbf{g}_k^T \mathbf{R}_{\mathbf{X}}^{-1} \mathbf{g}_k}
    \label{lcmv_filters}
\end{equation}

Можно показать, что в случае, если количество активных диполей меньше количества сенсоров,
а временные профили активации активных диполей некоррелированны,
такой фильтр будет выдавать нулевые значения для всех точек $k$, не совпадающих
с положениями активных диполей.
Такое поведение фильтра приводит к тому, что в случае, если точки дискретизации коры не
совпадают идеально с положениями активных диполей, такие диполи будут не видны при
фильтрации.
Кроме того, в случае малого количества активных источников такой фильтр
крайне плохо работает при наличии шума в данных, так как матрица $\mathbf{R}_{\mathbf{X}}^{-1}$
в этом случае является вырожденной (ее ранг совпадает с количеством активных диполей).

Чтобы исправить негативные эффекты этого фильтра, пользуются регуляризацией матрицы $\mathbf{R}_{\mathbf{X}}$:

\begin{equation}
    \mathbf{R}_{\mathbf{X}}^{reg} = \mathbf{R}_\mathbf{X} + \alpha \mathbf{I}
\end{equation}

Полученная матрица используется вместо матрицы $\mathbf{R}_\mathbf{X}$ в
уравнении~\ref{lcmv_filters}. Подстраивая коэффициент регуляризации $\alpha$, можно упрявлять степенью
сглаженности пространственной чувствительности фильтров $\mathbf{a}_k$.


Отметим, что решения для фильтров вида~\ref{lcmv_filters},
содержащие матрицу $\mathbf{R}_\mathbf{X}^{reg}$ вместо $\mathbf{R}_\mathbf{X}$ получаются для
видоизмененной версии исходной оптимизационной задачи~\ref{lcmv_objective_nonreg},
которую можно записать как

\begin{equation}
    \Expect{\norm{\mathbf{a}_k \mathbf{X}}^2} + \alpha \norm{\mathbf{a}_k}^2
    \rightarrow \underset{\mathbf{a}_k}{\min},
    s.t.: \mathbf{a}_k \mathbf{g}_k = 1
    \label{lcmv_objective_reg}
\end{equation}

Искомые выражения для фильтров $\mathbf{a}_k$ вновь могут быть получены методом множителей Лагранжа.
Второй член в оптимизируемом функционале~\ref{lcmv_objective_reg} накладывает ограничение на
норму искомых фильтров, не давая ей расти слишком сильно и исключая ту ситуацию,
когда оптимизация приводит к выбору фильтра, лежащего в подпространстве собственных
векторов матрицы $\mathbf{R}_\mathbf{X}$, соответствующих малым (шумовым) собственным числам.
В этом случае угол между фильтром и соответствующей топографией будет близок к $\pi/2$,
и чтобы удовлетворить ограничению, оптимизация будет компенсировать величину проекции
$\mathbf{a}_k$ на $\mathbf{g}_k$ за счет увеличения нормы $\mathbf{a}_k$.

Пространственные фильтры, использующие матрицу ковариации измерений,
обычно принятно называть адаптивными
(так как они не только используют знания о геометрии взаимного расположения коры и массива сенсоров,
но и подстраиваются под текущую активность) или
бимформерами (в дословном переводе~--- формирователь луча),
имея в виду, что они конструируют фильтры,
сфокуссированные на заданных пространственные направлениях, сродни лучу прожектора.

Рассмотрим теперь ту же самую задачу в векторной постановке.
Отметим сначала, что для скоррелированных источников процедура, описанная
выше, будет давать ошибочные значения мощностей сигналов в источниках.
Чтобы убедиться в этом, рассмотрим, как действуют фильтры на вектора прямой модели
в одномерном случае в предположении отсутствия шума,
а также считая, что количество активных источников $q$ меньше количества сенсоров $m$.
В таких предположениях матрица ковариации сигнала на сенсорах $\mathbf{R}_{\mathbf{X}}$
размером $m\times m$ будет иметь ранг $q$, и следовательно будет являться необратимой.
Тем не менее, ее можно приблизить псевдообратной матрицей.
Тогда, так как $\mathbf{R}_\mathbf{X} = \mathbf{G} \mathbf{R}_\mathbf{S} \mathbf{G}^T$

\begin{equation}
    \mathbf{R}_\mathbf{X}^{-1} \approx {(\mathbf{G} \mathbf{R}_\mathbf{X} \mathbf{G}^T)}^\dagger =
    {(\mathbf{G}^\dagger)}^T \mathbf{R}_\mathbf{S}^\dagger \mathbf{G}^\dagger = 
    {(\mathbf{G}^\dagger)}^T \mathbf{R}_\mathbf{S}^{-1} \mathbf{G}^\dagger
\end{equation}

Кроме того, так как $q < m$, псевдообратная для матрицы $\mathbf{G}$ будет также ее левой обратной:

\begin{equation}
    \mathbf{G}^\dagger \mathbf{G} = ({(\mathbf{G}^T \mathbf{G})}^{-1} \mathbf{G}^T) \mathbf{G} = \mathbf{I}
\end{equation}

А значит, для отдельных топографий активных диполей будет выполнено

\begin{equation}
    \mathbf{G}^\dagger \mathbf{g}_k = \mathbf{1}_k
\end{equation}

где $\mathbf{1}_k$ обозначает вектор, у которого на $k$-ом месте стоит единица,
а на всех остальных~--- нули.

Теперь, пользуясь формулой~\ref{lcmv_filters}, можем расписать произведение
$\mathbf{a}_k \mathbf{g}_l$:

\begin{equation}
    \mathbf{a}_k \mathbf{g}_l = \frac{\mathbf{g}_k^T \mathbf{R}_\mathbf{X}^{-1}\mathbf{g}_l}
                                   {\mathbf{g}_k^T \mathbf{R}_\mathbf{X}^{-1} \mathbf{g}_k}=
  \frac{{(\mathbf{G}^\dagger\mathbf{g}_k)}^T \mathbf{R}_\mathbf{S}^{-1} \mathbf{G}^\dagger \mathbf{g}_l}
       {{(\mathbf{G}^\dagger\mathbf{g}_k)}^T \mathbf{R}_\mathbf{S}^{-1} \mathbf{G}^\dagger \mathbf{g}_k}=
       \frac{\mathbf{1}_k \mathbf{R}_\mathbf{S}^{-1} \mathbf{1}_l}
            {\mathbf{1}_k \mathbf{R}_\mathbf{S}^{-1} \mathbf{1}_l} =
            \frac{\mathbf{R}_{\mathbf{S}, kl}^{-1}}
                 {\mathbf{R}_{\mathbf{S}, kk}^{-1}}
\end{equation}

Таким образом, на незашумленных измерениях в случае, если матрица $\mathbf{R}_\mathbf{S}$ диагональна
(а значит диагональна и $\mathbf{R}_\mathbf{S}^{-1}$),
то есть источники нескоррелированны, адаптивный пространственный фильтр
будет идеально восстанавливать источники, так как фильтр, направленный на $k$-ый источник
будет полностью подавлять сигнал, приходящий от всех источников, кроме $k$-го.
В этом случае произведение $\mathbf{a}_k \mathbf{g}_l = \delta_k^l$, где $\delta_k^l$~--- дельта Кронекера.

Вместе с тем, если источники обладают коррелированными временными рядами, неизбежно возникает искажение
восстановленного сигнала за счет протечки в точку, на которую направлен фильтр, сигнала от
этих коррелированных источников.

В случае, если их положение заранее известно, эту ситуацию можно исправить за счет дополнительных
линейных ограничений вида $\mathbf{a}_k \mathbf{g}_l=0$ для ограниченного заранее известного
набора индексов точек коры $l$.

Именно такая ситуация возникает при попытке сформулировать критерий оптимальной
адаптивной фильтрации в векторном случае. Действительно, для каждой точки коры мы будем
иметь идеально скоррелированные источники по трем направлениям ориентации токовых диполей,
так как они будут представлять собой проекции одного и того же вектора плотности тока на три разных
направления. С другой стороны, это один из тех редких случаев, когда мы знаем топорафии
источников с коррелированными временными рядами, поэтому можем явно выписать линейные ограничения
на оптимизируемый функционал. Для $i$-го из трех фильтров будем иметь:

\begin{equation}
    \Expect{\norm{\mathbf{a}_k^i \mathbf{X}}^2}
    \rightarrow \underset{\mathbf{a}_k}{\min},
    s.t.: \mathbf{a}_k^i \mathbf{g}_k^j =  \delta_i^j, j = 1,2,3
    \label{lcmv_opt_crit_components}
\end{equation}

В векторном виде, совокупно для трех фильтров, эта задача может быть переписана в виде


\begin{equation}
    \tr\left\{
        \mathbf{A}_k \mathbf{R}_\mathbf{X}\mathbf{A}_k^T
    \right\} \rightarrow \underset{\mathbf{A}_k}{\min},
    s.t.: \mathbf{A}_k \mathbf{G}_k =  \mathbf{I}
    \label{lcmv_opt_criterion_vec}
\end{equation}

Удобнее, однако, продолжить вывод покомпонентно.
Вновь выпишем лагранжиан для~\ref{lcmv_opt_crit_components} оптимизационной задачи:

\begin{equation}
    f_L(\mathbf{a}_k, \lambda) = \mathbf{a}_k^i \mathbf{R}_\mathbf{X} (\mathbf{a}_k^i)^T +
    \lambda (\mathbf{g}_k^T (\mathbf{a}_k^i)^T - \mathbf{1}_i)
\end{equation}

Дифференцируем и приравниваем к нулю:

\begin{equation}
    \frac{\partial f_L}{\partial \mathbf{a}_k^i} =
    2 \mathbf{R}_\mathbf{X} (\mathbf{a}_k^i)^T + \mathbf{g}_k \lambda^T = 0 \implies
    (\mathbf{a}_k^i)^T = - \frac{1}{2}\mathbf{R}_\mathbf{X}^{-1} \mathbf{g}_k \lambda^T
\end{equation}

Используя ограничение $\mathbf{g}_k^T (\mathbf{a}_k^i)^T = \mathbf{1}_i$, выразим $\lambda$:

\begin{equation}
    \mathbf{g}_k^T (\mathbf{a}_k^i)^T = \mathbf{1}_i =
    - \frac{1}{2}\mathbf{g}_k^T\mathbf{R}_\mathbf{X}^{-1} \mathbf{g}_k \lambda^T \implies
    \lambda^T = - 2{(\mathbf{g}_k^T\mathbf{R}_\mathbf{X}^{-1} \mathbf{g}_k)}^{-1} \mathbf{1}_i
\end{equation}

Окончательно, для $i$-го фильтра будем иметь

\begin{equation}
    (\mathbf{a}_k^i)^T = \mathbf{R}_\mathbf{X}^{-1} \mathbf{g}_k{(\mathbf{g}_k^T\mathbf{R}_\mathbf{X}^{-1} \mathbf{g}_k)}^{-1} \mathbf{1}_i
    \label{lcmv_filters_vec_comp}
\end{equation}

Или, комбинируя все три фильтра в одной векторной записи и транспонируя:

\begin{equation}
    \mathbf{A}_k =
    {(\mathbf{G}_k^T\mathbf{R}_\mathbf{X}^{-1} \mathbf{G}_k)}^{-1} \mathbf{G}_k^T \mathbf{R}_\mathbf{X}^{-1}
    \label{lcmv_filters_vec}
\end{equation}


\section{Методы глобальной оптимизации (методы минимальной нормы)}

К этой группе методов мы относим такие подходы решения обратной задачи,
которые основываются на минимизации глобального расхождения между измеренными данными
и данными, которые получаются при отображении на сенсоры восстановленных источников.
Отметим, что некоторые методы из этой группы также могут быть классифицированы как
пространственные фильтры, однако нам для дальнейшего изложения удобнее причислять их к
текущей группе.

Начнем рассмотрение с самого простого метода этого семейства~--- восстановления источников методом
псевдообратной матрицы.

\subsection{Умножение на псевдообратную матрицу}

Начнем с порождающей модели данных, предположив, что шума нет: 

\begin{equation}
    \vx(t) = \mG \vs(t) + \vomega(t)
    \label{gm_pinv}
\end{equation}

В случае, если количество активных источников меньше количества сенсоров,
а положения (а значит и топографии) этих источников известны, система
уравнений выше является переопределенной и может быть легко решена в смысле
минимизации среднеквадратичной ошибки.
Решение в этом случае широко известно и может быть выписано в явном виде
через псевдообратную матрицу, которая выписывается в явном виде как
$\mG^\dagger = {(\mG^T\mG)}^{-1}\mG^T$, поскольку $\mG^T\mG$ обратима:

\begin{equation*}
    \vs = \mG^\dagger\vx
\end{equation*}

На практике, однако, в задаче картирования мы имеем дело с противоположной ситуацией,
когда количество и положение реальных источников заранее неизвестно, и мы
покрываем кору равномерной сеткой точек, в каждой из которых мы пытаемся восстановить активность,
возможно нулевую. В этом случае количество неизвестных компонент вектора $\vs$ в уравнении~\ref{gm_pinv}
оказывается много больше количества доступных измерений,
и система уравнений оказывается недоопределенной, а значит имеет бесконечно много решений.

В этой ситуации представляется разумным выбрать в качестве решения системы~\ref{gm_pinv}
такое, которое обладает минимальной нормой.
На языке оптимизации такую задачу можно сформулировать как

\begin{equation}
    \frac{1}{2}\norm{\vs}^2 \rightarrow \underset{\vs}{\min},
    s.t.: \mG \vs = \vx
    \label{pinv_opt_criterion}
\end{equation}

Отметим вновь, что в случае обратной задачи МЭЭГ матрица $\mathbf{G}$ является
<<широкой>>, т.е.  количество ее столбцов больше, чем количество строк. Примем
здесь также, что столбцы матрицы $\mathbf{G}$ образуют полную систему векторов,
что как правило выполняется на практике при условии, что количество сенсоров
достаточно мало. В этом случае матрица $\mathbf{GG^T}$ является обратимой.

Выпишем функцию Лагранжа для оптимизационной задачи~\ref{pinv_opt_criterion}:

\begin{equation}
    f_L(\vec{s}, \lambda) = \frac{1}{2}\norm{\vs}^2 + \lambda \left(\mathbf{Gs - x}\right)
\end{equation}

Выпишем и приравняем частные производные лагранжиана по каждой из переменных:

\begin{gather}
    \frac{\partial f_L(\vec{s}, \lambda)}{\partial \lambda} = \mathbf{Gs - x} = 0
    \label{dL_first} \\
    \frac{\partial f_L(\vec{s}, \lambda)}{\partial \vec{s}} = \vec{s}
    + \mathbf{G}^T\vec{\lambda} = 0
    \label{dL_second}
\end{gather}

Далее выразим $\vec{s}$ из уравнения $\ref{dL_second}$ и подставим
полученное выражение в~\ref{dL_first}:

\begin{gather}
    \vec{s} = - \mathbf{G^T}\lambda
    \label{dL_second_s_expressed}\\
    \mathbf{Gs} = - \mathbf{G} \mathbf{G}^T\lambda = \vec{x}
    \label{dL_second_to_first}
\end{gather}

Из~\ref{dL_second_to_first} получим выражение для $\lambda$:

\begin{equation}
    \lambda = - {(\mathbf{G}\mathbf{G}^T)}^{-1}\vec{x}
    \label{pinv_lambda_expression}
\end{equation}

Наконец, подставив~\ref{pinv_lambda_expression} в~\ref{dL_second_s_expressed}, получим
окончательное выражение для $\vec{s}$:

\begin{equation}
    \vec{s} = \mathbf{G}^T{(\mathbf{GG}^T)}^{-1}\vec{x}
\end{equation}

Как видим, полученное решение имеет структуру псевдообратной матрицы,
полученной в явном виде для матрицы, столбцы которой образуют полную систему
векторов.

Мы показали таким образом, что псевдообратная матрица в обратной задаче МЭЭГ
производит решение, в точности удовлетворяющее исходной линейной системе, и при
этом обладающее минимальной нормой среди бесконечного множества таких решений.

Вместе с тем, несмотря на казалось бы очевидные преимущества такого способа
оценки источников в обратной задаче МЭЭГ, на практике описанный выше метод
никогда не используется. Это вызвано сочетанием двух обстоятельсв. 

Во-первых, реальные данные, как уже неоднократно упоминалось ранее, неизбежно
содержат шумовую компоненту, которую можно моделировать как некоторую добавку
$\omega$ к исходной линейной системе, с помощью которой мы будем оценивать
распределение источников $\vec{\hat{s}}$:

\begin{gather}
    \mathbf{G\hat{s}} = \vec{x}_{noisy}
    \label{original_linear_w_noise}\\
    \vec{x}_{noisy} = \vec{x} + \vec{\omega}
\end{gather}
Отметим, что на этот раз незашумленные измерения $\vec{x}$ представляют
собой некую математическую абстракцию"--- прямого доступа к этой величине мы
не имеем.

Во-вторых, хоть матрица $\mathbf{G}\mathbf{G}^T$ и является обратимой, она как
правило плохо обусловлена в силу того, что топографии (столбцы матрицы
$\mathbf{G}$) сильно коррелируют друг с другом.  Эти два фактора
приводят к тому, что вклад шумовой компоненты $\omega$ в оценку $\vec{s}$
значительно усиливается ввиду наличия больших собственных значений в матрице
${(\mathbf{G}\mathbf{G}^T)}^{-1}$:

\begin{gather*}
    \vec{\hat{s}} \stackrel{def}{=} \mathbf{G}^T{(\mathbf{G}\mathbf{G}^T)}^{-1}\vec{x}_{noisy} = 
    \mathbf{G}^T{(\mathbf{G}\mathbf{G}^T)}^{-1}\vec{x} +
    \mathbf{G}^T{(\mathbf{G}\mathbf{G}^T)}^{-1}\omega\\
    \vec{s} = \mathbf{G}^T{(\mathbf{G}\mathbf{G}^T)}^{-1}\vec{x}
    \\
    \vec{\hat{s}} =  \mathbf{s + G^T{(GG^T)}}^{-1}\omega
\end{gather*}

Иными словами, оценка $\vec{\hat{s}}$ перестает адекватно отражать реальное распределение
источников $\vec{s}$ из-за шумовой добавки $\mathbf{G^T{(GG^T)}}^{-1}\omega$, усиленной
неустойчивым обращением матрицы.

Очевидная модификация метода, направленного на устранение этого недостатка
посредством стабилизации обращения матрицы $\mathbf{GG^T}$ получила название
оценки методом минимальной нормы (Minimum Norm Estimate или сокращенно MNE)
(\cite{MNE_paper}).

\subsection{Оценка методом минимальной нормы (MNE).}

Оценка методом минимальной нормы является на сегодняшний день одним из наиболее
популярных алгоритмов анализа распределения источников в электрофизиологии, а
также представляет собой отправную точку для множества других методов, тем или
иным способом улучшающих его работу. К таким методам-модификациям относятся
прежде всего алгоритмы sLORETA и dSPM (\cite{sLORETA_paper, dSPM_paper}).

Суть алгоритма MNE, как уже было сказано, заключается в стабилизации обращения
матрицы $\mathbf{GG^T}$ посредством поднятия ее диагонали. Аналогичная идея
использовалась в уже описанном нами методе DICS (\ref{DICS_subsection}).
Оценка источников методом минимальной нормы таким образом задается соотношением

\begin{equation}
    \vec{\hat{s}} = \mathbf{G}^T(\mathbf{GG}^T + \alpha \mathbf{I}_m)^{-1}\vec{x}_{noisy},
    \label{MNE_estimation_formula}
\end{equation}

где $\alpha$"--- параметр регуляризации, а $\mathbf{I}_m$"--- единичная матрица
размера $m \times m$, где $m$"--- количество сенсоров. При должном выборе параметра
регуляризации $\alpha$ обращение матрицы $\mathbf{GG^T + \alpha I}_m$
становится теперь численно устойчивым, что ограничивает неконтролируемое
усиление шумовой компоненты, наблюдаемое для оценок, полученных умножением на
псевдообратную матрицу.

Отметим здесь, что формула~\ref{MNE_estimation_formula} может быть переписана в
эквивалентном виде:

\begin{equation}
    \vec{\hat{s}} = (\mathbf{G^TG} + \alpha \mathbf{I}_n)^{-1}\mathbf{G}^T\vec{x}_{noisy},
    \label{MNE_estimation_formula_v2}
\end{equation}

что легко проверить, подставив сингулярное разложением матрицы $\mathbf{G}$ в
обе формулы~\ref{MNE_estimation_formula}, \ref{MNE_estimation_formula_v2},
упростив полученные выражения и сравнив результаты.

При этом исходная зашумленная система теперь решается не в точности, а
приближенно; параметр $\alpha$ же регулирует баланс между
тем, насколько мала норма восстановленного решения и тем, насколько
хорошо это решение объясняет данные. Количественной оценкой последнего
является норма разности между решением, умноженным на прямую модель,
и вектором измерений.

Более формально, оценка~\ref{MNE_estimation_formula_v2} может быть найдена
как решение следующей задачи оптимизации:

\begin{equation}
    \frac{1}{2}\norm{\mathbf{G\hat{s} - x}_{noisy}}^2 + \frac{1}{2}\alpha\norm{\vec{\hat{s}}}^2 \rightarrow \underset{\vec{\hat{s}}}{\min}
    \label{MNE_optimization_crit}
\end{equation}

Действительно, дифференцируя функционал~\ref{MNE_optimization_crit} по $\vec{s}$,
получим:

\begin{gather*}
    \mathbf{G^T(G\hat{s} - x}_{noisy}) + \alpha\vec{\hat{s}} = 0\\
    \mathbf{G^TG\hat{s} + \alpha \hat{s} = G^Tx}_{noisy}\\
    \vec{\hat{s} = (G^TG + \alpha I_n)^{-1}G^Tx}_{noisy}
\end{gather*}


Таким образом становится понятен смысл метода минимальной нормы: с одной
стороны, метод пытается как можно точнее решить исходную систему уравнений, в
том числе и объясняя и шум в данных, а с другой он стремится ограничить норму
решения, не давая ему слишком сильно расти в попытках угнаться за объяснением
шумовой компоненты, которая требует значительного роста нормы решения в силу
плохой обусловленности матрицы $\mathbf{GG^T}$. Параметр регуляризации $\alpha$
при этом отвечает за баланс между этими двумя конкурирующими процессами.

Существует однако еще один способ взглянуть на метод минимальной нормы,
позволяющий как по-новому переосмыслить сам метод, так и существенно обобщить
его, сформировав определенный математический каркас, на основе которого
может быть сформулировано множество других методов поиска источников
в обратной задаче МЭЭГ. Этот способ заключается в описании обратной задачи
на языке байесовской вероятности.

Начнем с рассмотрения исходной системы уравнений~\ref{original_linear_w_noise}:

\begin{equation}
    \mathbf{G\hat{s}} = \vec{x}_{noisy} = \vec{x} + \omega
\end{equation}

При этом мы хотим найти оценку $\vec{\hat{s}}$ для величины $\vec{s}$,
для которой выполнено

\begin{equation}
    \mathbf{Gs} = \vec{x}.
\end{equation}

Предположим, что шум $\omega$ распределен нормально с единичной
матрицей ковариации и нулевым средним:

\begin{equation}
    \vec{Gs - x}_{noisy} = -\omega \sim \mathcal{N}(0, \mathbf{I}_m)
\end{equation}

Предположим также, что источники $\vec{s}$ некоррелированы и задаются
многомерным нормальным распределением с нулевым средним и дисперсией $\alpha$:

\begin{equation}
    \vec{s} \sim \mathcal{N}(0, \alpha\mathbf{I}_n)
\end{equation}

В этом случае критерий максимума апостериорной вероятности говорит нам, что
наиболee вероятным распределением (здесь пространственным, а не вероятностным)
источников $\vec{\hat{s}}$ с учетом наблюдаемых измерений
$\vec{x}_{noisy}$ будет такое, которое максимизирует апостериорную
плотность вероятности для величины $\vec{s}$:

\begin{equation}
    \vec{\hat{s}} =
    \underset{\vec{s}}{argmax} \left(
        e^{-\frac{1}{2}\norm{\vec{Gs - x}_{noisy}}^2} e^{-\frac{1}{2}\alpha\norm{\vec{s}}^2}
    \right)
    \label{MAP_crit_for_MNE}
\end{equation}

Логарифмируя соотношение~\ref{MAP_crit_for_MNE}, получим следующую формулу
для оценки $\vec{\hat{s}}$:

\begin{equation}
    \vec{\hat{s}} =
    \underset{\vec{s}}{argmin} \left(
        \frac{1}{2}\norm{\vec{Gs - x}_{noisy}}^2 + \frac{1}{2}\alpha\norm{\vec{s}}^2
    \right)
    \label{logMAP_crit_for_MNE}
\end{equation}

Как видим, критерий апостериорного максимума в точности привел нас
к задаче оптимизации~\ref{MNE_optimization_crit}, сформулированной для
метода MNE. На этот раз, однако, статистическая формулировка позволяет
нам увидеть метод MNE в перспективе.

Во-первых, становится ясна роль параметра регуляризации $\alpha$:
в вероятностной формулировке этот гиперпараметр вводится как дисперсия
источников при условии, что дисперсия шума на сенсорах равна единице,
то есть представляет собой квадрат отношения сигнал-шум.

Во-вторых, становится понятно, что выбор функционала качества для метода MNE
обусловлен предположениями о распределениях вероятностей шумовой компоненты и
источников. И если предположение о нормальном некоррелированном шуме можно
назвать естественным, то предположение о нормальности и некоррелированности
источников не кажется столь очевидным.  Немедленно возникает вопрос, как
изменятся свойства метода и можно ли его улучшить, если выбирать функцию
распределения априорной вероятности по-другому. В этом и состоит обобщающая
сила вероятностного взгляда на обратную задачу.

В действительности, аргументированный выбор функции распределения априорной
вероятности предполагает наличие определенных знаний о характере распределения
источников в конкретной задаче поиска активации по МЭЭГ измерениям. Источником
таких знаний может служить прежде всего информация об анатомическом строении
мозга испытуемого, а также знания о физиологических процессах, характерных для
рассматриваемой экспериментальной парадигмы.

Вместе с тем, функция распределения априорной вероятности может быть выбрана и из
общих соображений исходя из желаемых свойств решения. Дело в том, что одним из
широко известных свойств решений, полученных при помощи метода MNE, является
сглаживание источников по пространству. Речь идет о том, что в случае,
если истинная активация распределена в пространстве ``фокально'', то есть
по относительно небольшому участку коры, оценка этой активации методом
MNE как правило распределена по области, своим размером сильно превышающей
истинную, затрудняя таким образом оценку мощности источника сигнала в
случаях, когда несколько источников находятся близко друг к другу.
Кроме того сильным пространственным сглаживанием осложнена оценка фазовой
связности источников, восстановленных методом MNE, в силу высокой вероятности
взаимной протечки сигнала.

С другой стороны здравый смысл подсказывает нам, что в случае поиска источника,
сосредоточенного в одной точке, наиболее простым решением, объясняющим
наблюдения, должен быть единственный токовый диполь, расположенный в этой
точке. Однако метод MNE в силу пространственного сглаживания будет всегда
давать оценку, рассредоточенную пятном вблизи точки, которая действительно была
активна, т.е. оценка будет содержать целую группу активных диполей.
Такое положение веще наводит на мысль, что руководствуясь принципом
максимальной простоты решения, мы должны пересмотреть наш критерий простоты.

При формулировке метода псевдообратной матрицы, а также метода MNE мы
отталкивались от минимизации $L2$-нормы решения. Очевидным решением описанной
выше проблемы является модификация критерия: мы должны минимизировать
количество ненулевых компонент в векторе источников $\vec{s}$. Такой
критерий формализуется как минимизация $L_0$-"нормы" решения (в действительности $L_0$ не является нормой,
т.к. для нее нарушается свойство однородности). Так как решение в то же время
должно минимизировать норму невязки $\norm{\vec{Gs - x}}_{L_2}$, функционал
качества решения по аналогии с~\ref{MNE_optimization_crit} запишется в виде

\begin{equation}
    \frac{1}{2}\norm{\mathbf{G\hat{s} - x}_{noisy}}^2_{L_2} + \alpha\norm{\vec{\hat{s}}}_{L_0}
    \rightarrow \underset{\vec{\hat{s}}}{\min}.
    \label{l0_optimization_crit}
\end{equation}

В вероятностной постановке такой оптимизационный критерий соответствует
выбору в качестве априорной функции распределения многомерного распределения
Бернулли в предположении, что все источники независимы. Иными словами,
для каждого источника предполагается, что он активен с вероятностью
$p$ и не активен с вероятностью $1-p$, а вероятность совместной активации
любого набора диполей-источников равна произведению вероятностей активации
каждого из них в отдельности.
Для отдельно взятого источника с индексом
$i$ такая функция распределения будет выглядеть как

\begin{equation}
    P_{s_i} = p^{(1 - \delta(s_i))} (1-p)^{\delta(s_i)}
\end{equation}

где $\delta(s_i)$ представляет собой дельта-функцию. Эта формула представляет
собой обобщение функции плотности вероятности для одного испытания Бернулли
на случай непрерывной переменной.

Для совокупной активации с учетом независимости активаций отдельных диполей
(предполагается, что активации всех диполей равновероятны с вероятностью $p$)
функция распределения будет выглядеть как произведение функций распределения
для компонент вектора $\vec{s}$. Выпишем ее и приведем к форме функции
распределения из экспоненциального семейства, чтобы ее было удобнее использовать
в критерии максимума апостериори:

\begin{multline}
    P_{\vec{s}} = \prod_{i=1}^{N} p^{(1-\delta(s_i))} (1-p)^{\delta(s_i)}=
    \exp\left(\sum_{i=1}^N \left(\log(p^{1-\delta(s_i)}) + \log((1-p)^{\delta(s_i)})\right)\right)=\\
    =\exp\left(\sum_{i=1}^N \left(\log(p) - \delta(s_i) \log(p) + \delta(s_i)\log(1-p)\right)\right)=\\
    =\exp\left(-\sum_{i=1}^N \left(\delta(s_i) \log\left(\frac{p}{1-p}\right) - \log(p)\right)\right) = \\
    =\exp\left(-\log\left(\frac{p}{1-p}\right) \norm{\vec{s}}_{L_0} + N\log(p)\right)
    \label{l0_multivariate_bernoulli_distribution}
\end{multline}

Записывая для априорного распределения
~\ref{l0_multivariate_bernoulli_distribution} критерий максимума апостериори,
логарифмируя и учитывая, что $N\log(p)$ не зависит от $\vec{s}$, получим
оптимизационный критерий~\ref{l0_optimization_crit}. Отметим, что вероятностная
формулировка позволяет в явном виде получить выражение для параметра $\alpha$
через вероятность активации одного источника:

\begin{equation}
    \alpha = \log\left(\frac{p}{1-p}\right)
\end{equation}


Проблема с задачей оптимизации~\ref{l0_optimization_crit} заключается в том,
что она является NP-сложной, а значит на практике метод, основанный на
оптимизации функционала~\ref{l0_optimization_crit} использовать не получится.

Тем не менее, существуют подходы к решению обратной задачи,
основанные на аппроксимации $L_0$-нормы таким образом, чтобы
приближенная задача оптимизации стала алгоритмически разрешимой.

Хронологически первым методом решения обратной задачи для МЭЭГ,
основанным на этой идее стал метод минимальных токов
(Minimum Current Estimate, MCE).

\subsection{Метод минимальных токов (MCE)}

В своей основе суть метода заключается в аппроксимации дельта-функции,
используемой в "норме" $L_0$ из функционала~\ref{l0_optimization_crit} модулем,
соответствующим норме $L_1$ и решении возникающей задачи оптимизации. Выбор
нормы $L_1$ при формулировке метода через критерий максимума апостериори
соответствует выбору в качестве априорной функции распределения источников
распределения Лапласа.


Метод MCE в силу специфического выбора функции распределения источников
позволяет получать спарсные решения, т.е. решения, сильно разреженные по
пространству, демонстрируя свойства аналогичного ему метода
LASSO~\cite{lasso_paper}, используемого в теории оценивания и машинном обучении.
Как и метод LASSO, метод MCE стремится обнулить максимальное количество
компонент вектора $\vec{s}$, чего и следует ожидать от метода, основанного
на аппроксимации нормы $L_0$.

Оценка источников методом минимальных токов была
предложена в статье~\cite{Uutela_MCE}, и методологически является важным алгоритмом
давшим развитие ряду более совершенных методов. Тем не менее, на практике этот
метод сейчас практически не используется несмотря на выгодное для определенных
приложений свойство спарсности решений, что обусловлено рядом присущих ему
недостатков.

Во-первых, метод является более требовательным к вычислениям по сравнению с
MNE, так как формула оценки методом MCE не может быть выписана в явном виде и
решение необходимо получать численно. Кроме того, оптимизируемый функционал
качества больше не является гладкой функцией, что затрудняет построение
эффективной численной схемы.  Иными словами, расплатой за свойство
пространственной разреженности решения являются сложности с поиском экстремума
функционала качества.

Во-вторых, метод никак не учитывает временную динамику сигнала, так как поиск
решения происходит для каждого временного среза отдельно.  На практике это
приводит к тому, что временные ряды, восстановленные для источников методом
MCE, не являются гладкими: для соседних временных срезов могут быть обнулены
абсолютно разные компоненты вектора $\vec{s}$. Здесь стоит отметить, что
хотя метод MNE также не учитывает временную структуру сигнала явно, он, тем не
менее, сохраняет гладкость сигнала по времени, так как представляет собой
лишь умножение сигнала на невырожденную матрицу.

Чтобы убедиться в том, что метод MCE нарушает временную гладкость сигнала,
рассмотрим, как будет выглядеть его функционал качества для каждого среза по
времени $t$:

\begin{equation}
    \frac{1}{2}\norm{\mathbf{G\hat{s}}(t) - \vec{x}_{noisy}(t)}^2_{L_2} + \alpha\sum_i\abs{\hat{s_i}(t)}
    \rightarrow \underset{\vec{\hat{s}}}{\min}.
    \label{l1_optimization_crit}
\end{equation}

Так как минимум для каждого среза ищется независимо от других, а квадрат нормы $L_2$ и
норма $L_1$ являются аддитивными функциями временных срезов,
для совокупности всех временных срезов, для которых
решается обратная задача (их количество обозначим $T$), эквивалентный
функционал качества можно записать как

\begin{equation}
    \frac{1}{2}\norm{\mathbf{G\hat{S}} - \mathbf{X}_{noisy}}^2_{fro} + \alpha\sum_{i,j}\abs{\hat{\matr{S}}_{i,j}}
    \rightarrow \underset{\matr{\hat{S}}}{\min},
    \label{l1_optimization_crit_matrix}
\end{equation}
где $\mathbf{\hat{S}}$ --- матрица размером $N\times T$, содержащая по строкам
временные ряды, оцененные для каждого источника, а $\mathbf{X}_{noisy}$ ---
матрица размером $M \times T$ временных рядов, записанных сенсорами. 

Такая запись позволяет увидеть, что при оптимизации
функционала~\ref{l1_optimization_crit_matrix} компоненты вектора $\vec{s}$
штрафуются от временного среза к временному срезу при помощи нормы $L_1$ точно
также, как и по пространству, то есть свойство разреженности решения,
сопутствующее этой норме, применяется не только к пространственной структуре
восстановленного сигнала, но и к его временной динамике. В итоге решения,
полученные методом MCE, оказываются крайне нестабильными. Аналогично для метода
MNE решение штрафуется и по пространству и по времени нормой $L_2$, соответствующей
гладким решениям.

Вместе с тем временная спарсность противоречит нашей интуиции относительно
природы сигнала, так как допускает, что соседние временные срезы могут
соответствовать абсолютно разному пространственному распределению источников
тогда как мы ожидаем определенной гладкости во временной динамике источников,
поскольку характерные времена процессов, порождающих на коре
измеримый сигнал (десятки миллисекунд), сравнительно велики по отношению к типичным временам
сэмплирования (десятые доли миллисекунды).

Отсюда возникает желание совместить пространственную спарсность метода
MCE со временной гладкостью метода MNE. Эта идея приводит к группе методов,
основанных на комбинации норм, штрафующих матрицу решения $\mathbf{S}$
неодинаково по времени и пространству. Методы, основанные на этой идее получили
название Mixed Norm Estimate (MxNE), или оценка методом смешанной нормы. Их мы и
рассмотрим далее.

\subsection{Оценка методом смешанной нормы.}
\label{section_mixed_norm}

Оценка методом смешанной нормы, как уже было сказано, основана на
комбинировании различных норм в регуляризационном слагаемом, штрафующих строки
и столбцы матрицы $\mathbf{S}$ по-разному.  Эта идея формализуется при помощи понятия
\emph{смешанной нормы}, которая для матрицы $\mathbf{X}$ определяется как:

\begin{equation}
    \norm{\mathbf{X}}_{p,q} \stackrel{def}{=}
    \left(\sum_i \left( \sum_j X_{ij}^p \right)^{\frac{q}{p}}\right)^{\frac{1}{q}}
    \label{mixed_norm_def}
\end{equation}

Как видим, смешанная норма представляет собой последовательное вычисление
норм $L_p$ и $L_q$, при этом вторая норма берется от вектора-столбца значений первой
нормы на строках матрицы $\mathbf{X}$.

Использование смешанной нормы вместо обычной в регуляризационном слагаемом
функционала качества позволяет получать решения с желаемыми свойствами
пространственной спарсности и временной гладкости в случае, если первая штрафующая норма
порождает гладкие решения (например, $L_2$), а вторая --- спарсные (например, $L_1$).

Такой подход однако требует дополнительных усилий при численной оптимизации
полученного функционала качества, так как он, как и в случае метода MCE,
представляет собой негладкую функцию от активаций на коре.

Свойство разреженности порождаемых решений при этом может быть дополнительно
усилено в случае, если в качестве второй нормы мы используем квазинорму $L_q$ с
коэффициентом $q$, принимающим значения в промежутке от 0 до 1. В этом случае
за дополнительную спарсность придется расплачиваться еще большими сложностями в
оптимизации функционала качества, так как при использовании дробной квазинормы
целевой функционал не только не является гладким, но также не является
выпуклым. Тем не менее, такая оптимизация, как мы рассмотрим далее, возможна,
хоть и не гарантирована от сходимости к локальным минимумам.

Но сначала рассмотрим стратегию оптимизации функционала, получающегося при
использовании смешанных норм, обладающих спарсностью по пространству, для нормы
$L_{21}$.

\subsubsection{Оптимизация функционала с нормой $L_{21}$}

При оптимизация функционала с нормами, порождающими спарсные решения, такими
как норма $L_1$, решение в общем случае не может быть получено в явном виде,
что уже отмечалось ранее.  Тем не менее, в частном случае регуляризации нормой
$L_1$, когда матрица прямой модели является единичной, оказывается возможным
выразить решение в явном виде~\cite{Selensick_sparse_signal_restoration} (в
этом случае задача фактически сводится к удалению шума из вектора наблюдений
$\vec{x}$). Рассмотрим этот случай, так как он поможет нам построить
итерационную схему для общего случая с произвольной матрицей $\mathbf{G}$ и
рассмотреть обобщение этой схемы на случай смешанной нормы, полученное в
статье~\cite{gramfort_2012}. Запишем сначала функционал качества:

\begin{equation}
    f(\vec{\hat{s}}) = \frac{1}{2}\norm{\vec{\hat{s} - x}_{noisy}}^2_{L_2} + \alpha\norm{\vec{\hat{s}}}_{L_1}
    \rightarrow \underset{\vec{\hat{s}}}{\min}.
    \label{eq:denoising_crit}
\end{equation}

Полученный функционал, как нетрудно заметить, распадается на отдельные
неотрицательные слагаемые, каждое из которых зависит только от одной компонеты
вектора $\vec{\hat{s}}$, а значит минимизация всего
функционала~\ref{eq:denoising_crit} достигается минимизацией каждого из слагаемых
вида

\begin{equation}
    y_i = \frac{1}{2}{(\hat{s}_i - x_i)} ^ 2 + \alpha \abs{\hat{s}_i}
    \label{eq:componentwise_soft_thresh_crit}
\end{equation}


Критерием оптимума выражения \ref{eq:componentwise_soft_thresh_crit} является принадлежность нуля
его субдифференциалу: $0 \in \partial y_i$.
Для каждого из слагаемых $y_i$ везде кроме $\hat{s}_i = 0$ выражение \ref{eq:componentwise_soft_thresh_crit} является дифференцируемым.
Поэтому для $\hat{s}_i \neq 0$ условие $0 \in \partial y_i$ эквивалентно равенству нулю производной по $\hat{s_i}$:
\begin{equation}
    \frac{\D y_i}{\D \hat{s_i}} = (\hat{s_i} - x_i) + \alpha \sign(\hat{s_i}) = 0\\
\end{equation}

С учетом ограниений на знак $\hat{s}_i$ получим, что $\hat{s}_i = x_i - \alpha$, если
$x_i > \alpha$, и $\hat{s}_i = x_i + \alpha$, если $x_i < \alpha$

В случае $\hat{s}_i = 0$ субдифференциал \ref{eq:componentwise_soft_thresh_crit} является отрезком
$\left[-x_i - \alpha, -x_i + \alpha\right]$. Этот отрезок содержит ноль тогда и только тогда, когда
$\abs{x_i} \leq \alpha$. То есть, если $\abs{x_i} \leq \alpha$, значение $\hat{s}_i = 0$ является
оптимальным.

Окончательно получим, что для $\hat{s}_i$ возможно три значения
в зависимости от того, к какому промежутку принадлежит $x_i$:
\begin{equation}
    \hat{s_i}=h(x_i, \alpha) =
    \begin{cases}
        x_i - \alpha,   &  x_i > \alpha\\
        0,              &  \abs{x_i} \leq \alpha\\
        x_i + \alpha,   &  x_i < -\alpha
    \end{cases}
\end{equation}

Отметим также, что оператор $h(x, c)$ может быть переписан в более компактной форме:

\begin{equation}
    h(x, c) = x\left(1 - \frac{c}{\abs{x}}\right)^{+} \text{, где } (\cdot)^{+} \stackrel{def}{=} \max(\cdot, 0)
    \label{eq:soft_thresh_compact}
\end{equation}

Таким образом, в рассмотренной нами простейшей постановке применение
$L_1$-регуляризатора порождает решения, которые получаются из измерений
покомпонентным применением оператора $h(x, c)$, который обнуляет компоненты
вектора измерений, не превосходящие по амплитуде порог $c$.  Такой оператор
известен в литературе под названием soft thresholding.

\subsubsection{Оптимизация методом ISTA.}
\label{subsubsec:ista}

Теперь перейдем к рассмотрению общего случая с произвольной матрицей
$\mathbf{G}$.  Одним из способов оптимизации функционала качества в этом случае
является MM-алгоритм (majorization-minimization), позволяющий для выпуклых но
негладких функций строить итерационные схемы оптимизации, использующие более
простую целевую функцию. Получающийся в итоге алгоритм получил название
ISTA (iterative shrinkage thresholding algorithm).

При построении MM-алгоритма для целевой функции $f(\vec{s})$ и некоторой
точки $\vec{\hat{s}}^{(k)}$ строится \emph{мажоризирующая} функция $g^{(k)}$, совпадающая
с функцией $f$ в точке $\vec{\hat{s}}^{(k)}$:

\begin{gather*}
    g^{(k)}(\vec{s}) \geq f(\vec{s})\\
    g^{(k)}(\vec{\hat{s}}^{(k)}) = f(\vec{\hat{s}}^{(k)})
\end{gather*}

При этом следующее приближение к решению $\vec{\hat{s}}^{(k+1)}$ находится как
минимум мажоризирующей функции, построенной для точки $\vec{\hat{s}}^{(k)}$. Такая
итерационная процедура обладает монотонной сходимостью к точке минимума
функционала $f$, при условии, что этот функционал является выпуклым
\cite{combettes}.  Конкретный выбор последовательности функций $g^{(k)}$ при этом
делается исходя из удобства минимизации.

Чтобы получить функцию, обладающую искомыми свойствами, достаточно добавить к
исходной функции $f$ неотрицательное слагаемое, которое обращается в ноль в
точке $\vec{\hat{s}}^{(k)}$.  В качестве такого слагаемого возьмем выражение
$\frac{1}{2}(\vec{s} - \vec{\hat{s}}^{(k)})^T (\beta\mathbf{I - G^TG})(\vec{s} - \vec{\hat{s}}^{(k)})$,
выбрав для $\beta$ значение, превосходящее максимальное собственное число матрицы
$\mathbf{G^TG}$, чтобы гарантировать положительность. Раскрыв скобки и приведя подобные
слагаемые и выделив полный квадрат, получим для $g^{(k)}$ выражение

\begin{equation}
    g_k(\vec{s}) =
    \frac{1}{2}\norm{\vec{\hat{s}}^{(k)} + \frac{1}{\beta} \matr{G}^T(\vec{x}_{noisy} - \matr{G} \hat{\vec{s}}^{(k)}) - \vec{s}}^2_{L_2} + \frac{\alpha}{\beta}\norm{\vec{s}}_{L_1} + C,
\end{equation}

где в $C$ сгруппированы слагаемые, которые не зависят от $\vec{s}$ и
следовательно не влияют на минимизацию. Отметим, что в полученной функции
$g^{(k)}$ аргумент $\vec{s}$ присутствует под знаком нормы без умножения на
матрицу $\mathbf{G}$, то есть для этой функции задача поиска минимума сводится
к частному случаю единичной матрицы прямой модели, рассмотреному нами выше, и
ее минимум может быть найден в явном виде как результат применения оператора
$h(x, c)$ к компонентам вектора $\vec{\hat{s}}^{(k)} + \frac{1}{\beta} \matr{G}^T(\vec{x}_{noisy} - \matr{G} \vec{\hat{s}}^{(k)})$:

\begin{equation}
    \vec{\hat{s}}^{(k+1)} = h\left(\vec{\hat{s}}^{(k)} + \frac{1}{\beta} \matr{G}^T(\vec{x}_{noisy} - \matr{G} \vec{\hat{s}}^{(k)}), \frac{\alpha}{\beta}\right)
    \label{eq:fista_next_iter_soft_thresh}
\end{equation}

Согласно свойствам MM-алгоритма полученная последовательность оценок
$\vec{\hat{s}}^{(k)}$ сходится к искомому минимуму функционала $f$.

Отметим, что при посторении мажоранты для функционала качества конкретный вид
штрафующей нормы никак не учитывался; тип используемой нормы сказывается только
при подсчете оператора $h$ после того как, используя метод MM, мы распутывали
переменные под знаком нормы, сводя задачу к случаю единичной матрицы.

На самом деле полученный алгоритм на каждой итерации можно условно разбить на два базовых
шага: 

\begin{enumerate}
    \item градиентный спуск для $L_2$-нормы ошибки в объяснении измерений;
        величина $\beta$ при этом регулирует скорость градиентного спуска и ограничена
        снизу максимальным собственным значением оператора $\matr{G^TG}$
        Этот шаг не зависит от конкретного вида штрафующей нормы.
    \item удаление шума для вновь полученной градиентным спуском точки с
        использованием априорного знания о сигнале, кодируемого нормой $L_1$.
\end{enumerate}

Это наблюдение мотивирует нас обобщить оператор soft thresholding на
более широкий класс функций-регуляризаторов. В общем случае
при оптимизации суммы двух функций

\begin{equation}
    f_1(\vec{x}) + \alpha f_2(\vec{x}) \rightarrow \underset{\vec{x}}\min,
\end{equation}

где обе функции $f_1, f_2$ являются выпуклыми, а функция $f_1$ дополнительно
является гладкой, вводится понятие \emph{оператора приближения} (proximity
operator или proximal operator):

\begin{equation}
    \prox_{c f_2}(\vec{y}) = \argmin_{\vec{x}}\left(\frac{1}{2}\norm{\vec{x - y}}^2_{L_2} + c f_2(\vec{x})\right)
    \label{eq:prox_operator_def}
\end{equation}

Как и для~\ref{eq:denoising_crit} выражение~\ref{eq:prox_operator_def} можно
трактовать как оператор удаления шума из вектора $\vec{x}$ с учетом априорного
знания о структуре сигнала, которая кодируется при помощи регуляризатора $f_2$.
Отметим также, что при выборе $f_2 = \norm{\cdot}_{L_1}$ мы получим в точности
функционал~\ref{eq:denoising_crit}, который приводит к формуле оператора
soft thresholding.

Используя оператор приближения, для функционалов вида

\begin{equation}
    f(\vec{\hat{s}}) = \frac{1}{2}\norm{\vec{x}_{noisy} - \matr{G}\vec{\hat{s}}}^2_{L_2} + \alpha f_{norm}(\vec{\hat{s}})
    \rightarrow \underset{\vec{\hat{s}}}{\min}.
    \label{eq:abstract_reg_func_crit}
\end{equation}

можем выписать в общем виде выражение для вычисления следующего шага
MM-алгоритма с произвольным выпуклым регуляризатором $f_{norm}$:

\begin{equation}
    \vec{\hat{s}}^{(k+1)} =
    \prox_{\frac{\alpha}{\beta} f_{norm}}\left(\vec{\hat{s}}^{(k)} + \frac{1}{\beta} \matr{G}^T(\vec{x}_{noisy} - \matr{G} \vec{\hat{s}}^{(k)})\right)
    \label{eq:mm_next_iter_prox}
\end{equation}

Получим теперь в явном виде оператор приближения для регуляризатора,
использующего смешанную норму~\cite{gramfort_2012} с учетом того, что теперь
вместо векторов $\vec{x}, \vec{y}$ необходимо использовать матрицы $\matr{S},
\matr{X}$. Выпишем сначала определение:

\begin{multline}
    \prox_{c \norm{\cdot}_{L_{21}}}(\matr{\tilde{S}}) =
    \argmin_{\matr{S}}\left(\frac{1}{2}\norm{\matr{S - \tilde{S}}}^2_{fro} + c \norm{\matr{S}}_{L_{21}}\right)=\\
    \argmin_{\matr{S}}\left(\frac{1}{2}\norm{\matr{S - \tilde{S}}}^2_{fro} + c \sum_i \norm{\matr{S}_{i,:}}_{L_2}\right),
    \label{eq:def_proximity_for_l21}
\end{multline}
где $\matr{S}_{i,:}$ обозначает $i$-ую строку матрицы $\matr{S}$ или, что то же
самое, временной ряд $i$-го источника.

Минимум выражения в скобках достигается в тех точках, в которых субдифференциал
выражения содержит нулевой вектор.

В случае $\matr{S}_i = \vec{0}$ блоки субдифференцала нормы $L_{21}$,
соответствующие строкам матрицы $\matr{S}$,
будут состоять из всех
векторов $\vec{d}$, которые удовлетворяют соотношению

\begin{equation}
    \norm{\vec{x}}_{L_2} \geq \vec{d}^T\vec{x}
    \label{eq:subdiff_for_norm}
\end{equation}
для любого вектора $\vec{x}$. Максимум выражения $\vec{d}^T\vec{x}$ достигается, когда
$\vec{x}$ коллинеарен $\vec{d}$. Тогда условие \ref{eq:subdiff_for_norm} перепишется
как 
\begin{equation}
    \norm{\vec{d}}_{L_2} \leq 1,
\end{equation}
т.е. соответствующие блоки субдифференциала состоят из векторов, $L_2$-норма которых
не превосходит 1.

Теперь условие принадлежности нуля соответствующему блоку субдифференциала может быть записано как
\begin{equation}
    \tilde{\matr{S}}_i + c\vec{d} = \vec{0},
\end{equation}
где $\vec{d}$ --- некоторый вектор, соответствующий блоку субдифференциала.
Такой вектор найдётся при условии, что
\begin{equation}
    \norm{\tilde{\matr{S}}_i}_{L_2} \leq c,
\end{equation}
т.е. решение $\matr{S}_i = \vec{0}$ минимизирует функционал \ref{eq:def_proximity_for_l21} по соответствующим компонентам
тогда и только тогда, когда $\norm{\tilde{\matr{S}}_i}_{L_2} \leq c$.

Если $\norm{\matr{S}_{i,:}} \neq \vec{0}$, выражение в скобках является дифференцируемым
по соответствующим компонентам, поэтому для поиска минимума достаточно приравнять
к нулю производную по компонентам вектора $\matr{S}_i$:
% Чтобы найти минимум выражения в скобках, продифференцируем его по компонентам
% матрицы $\matr{S}$ и приравняем производную к нулю везде, где $\norm{\matr{S}_{i,:}} \neq 0$
% (если это не выполняется, для компонент вектора $\matr{\tilde{S}}_{i,:}$ получаем тривиальное решение):

\begin{gather}
    (s_{ij} - \tilde{s}_{ij}) + c \frac{s_{ij}}{\norm{\matr{S}_{i,:}}} = 0\\
    \tilde{s}_{ij} = s_{ij} \left(1 + \frac{c}{\norm{\matr{S}_{i,:}}}\right)
    \label{eq:intermediate_prox_l21}
\end{gather}

Зафиксировав индекс $k$, для каждого индекса $l$ возведем последнее равенство
в квадрат и сложим результаты, чтобы выразить норму $\norm{\matr{S}_{i,:}}$ через норму
$\norm{\matr{\tilde{S}}_{i,:}}$:

\begin{equation}
    \norm{\matr{\tilde{S}_{i,:}}}^2 =
    \norm{\matr{S}_{i,:}}^2 \left( 1 + \frac{c}{\norm{\matr{S}_{i,:}}}\right)^2 =
    \left( \norm{\matr{S}_{i,:}}+ c\right)^2
\end{equation}

Откуда
\begin{equation}
    \norm{\matr{S}_{i,:}} = \norm{\matr{\tilde{S}}_{i,:}} - c
    \label{eq:intermediate1_prox_l21}
\end{equation}

Подставляя~\ref{eq:intermediate1_prox_l21} в \ref{eq:intermediate_prox_l21}, а
также учитывая, что так как $\norm{\matr{S}_{i,:}} > 0$, всегда выполнено
$\norm{\matr{\tilde{S}}_{i,:}} > c$, получим

\begin{equation}
    s_{ij} = \tilde{s}_{ij} \left( 1 - \frac{c}{\norm{\matr{\tilde{S}}_{i,:}}} \right)^{+}
    \text{, где } (\cdot)^{+} \stackrel{def}{=} \max(\cdot, 0)
\end{equation}


Окончательно будем иметь

\begin{equation}
    \prox_{c\norm{\cdot}_{L_{21}}}(\matr{\tilde{S}}) =
    \diag \set[\Bigg]{\left( 1 - \frac{c}{\norm{\matr{\tilde{S}}_{i,:}}} \right) ^ {+}}_i \matr{\tilde{S}}
    \label{eq:proximity_operator_for_l21}
\end{equation}

Применение оператора~\ref{eq:proximity_operator_for_l21} к правилу обновления
целевой переменной~\ref{eq:mm_next_iter_prox} позволяет сформулировать
алгоритм ISTA, обобщенный на случай регуляризации со смешанной нормой.


\subsubsection{Контроль сходимости схемы ISTA.}
Остановка итерационной схемы ISTA возможна в соответствии с эвристикой,
часто используемой на практике при оптимизации того или иного функционала,
которая заключается в контроле нормы разности решения на текущем и предыдущем шаге.
Если эта норма опускается ниже некоторого заранее заданного порога, алгоритм
останавливается.

В некоторых случаях, однако, оказывается возможным сформулировать более
обоснованный критерий останова, основанный на теории двойственности для
задач оптимизации и использующий понятие \emph{разрыва двойственности}
как средство контроля оптимальности найденного решения.

Как раз такой критерий останова использовали авторы статьи~\cite{gramfort_2012}.
Разберем подробнее суть подхода к контролю оптимальности найденного численно
решения на основе теории двойственности.

Выпишем задачу оптимизации с нормой $L_{21}$:
\begin{equation}
    \frac{1}{2}\norm{\matr{X}_{noisy} - \matr{G}\matr{S}}^2_{fro} + \alpha \norm{\matr{S}}_{L_{21}}
    \rightarrow \underset{\matr{S}}{\min}.
    \label{eq:l21_reg_func_crit}
\end{equation}

Перепишем эту задачу в более общем виде:

\begin{equation}
    f_1(\matr{S}) - f_2(\matr{G}\matr{S}) \rightarrow \min_{\matr{S}}
    \label{eq:dual_init_crit}
\end{equation}

Здесь функция $f_1(\matr{S})=\alpha \norm{\matr{S}}_{L_{21}}$ является
выпуклой, а функция $f_2(\matr{V})=\frac{1}{2}\norm{\matr{X}_{noisy} - \matr{V}}^2_{fro}$ --- вогнутой.
Кроме того, $f_1$ и $f_2\circ\matr{G}$ определены всюду на $\mathbb{R}^{n\times m}$.
При этом относительно дифференцируемости функций $f_1$, $f_2$ мы дополнительно
ничего не предполагаем.

Можем переписать исходную задачу оптимизации как задачу оптимизации с
ограничениями-равенствами:

\begin{equation}
    \begin{gathered}
        f_1(\matr{S}) - f_2(\matr{V}) \rightarrow \min_{\matr{S},\matr{V}}\\
        \text{s.t.: } \matr{V} = \matr{G}\matr{S}
    \end{gathered}
    \label{eq:dual_crit_split}
\end{equation}


Для этой оптимизационной задачи~\ref{eq:dual_crit_split} можем выписать функцию Лагранжа:

\begin{equation}
    L(\matr{S}, \matr{V}, \matr{\Lambda}) = f_1(\matr{S}) - f_2(\matr{V}) + \trace(\matr{\Lambda}^T(\matr{V} - \matr{G}\matr{S}))
    \label{eq:dual_lagrangian}
\end{equation}

Для функции Лагранжа в теории выпуклой оптимизации двойственная
функция определяется как функция от множителей Лагранжа,
равная инфимуму от функции Лагранжа в каждой точке:

\begin{equation}
    g(\matr{\Lambda}) = \inf_{\matr{S}, \matr{V}} L(\matr{S}, \matr{V}, \matr{\Lambda})
    \label{eq:dual_dual_func_def}
\end{equation}

Распишем, чему равна двойственная функция для Лагранжиана~\ref{eq:dual_lagrangian}:

\begin{multline}
    g(\matr{\Lambda}) = \inf_{\matr{S}, \matr{V}} (f_1(\matr{S}) - f_2(\matr{V}) - \trace(\matr{\Lambda}^T(\matr{G}\matr{S} - \matr{V}))) =\\
    = \inf_{\matr{S}, \matr{V}} \left( (\trace(\matr{\Lambda}^T \matr{V}) - f_2(\matr{V})) - (\trace(\matr{\Lambda}^T \matr{G}\matr{S}) - f_1(\matr{S}))\right) =\\
    = \inf_{\matr{V}}(\trace(\matr{\Lambda}^T \matr{V}) - f_2(\matr{V})) - \sup_{\matr{S}}((\trace(\matr{G}^T\matr{\Lambda})^T \matr{S}) - f_1(\matr{S}))
    \label{eq:dual_derive_lagrangian}
\end{multline}

Как видим, двойственная функция распалась на две сходные по форме компоненты,
каждая из которых требует поиска оптимума по своей переменной.


В теории выпуклой оптимизации рассматривают выпуклую и вогнутую сопряженные функции,
которые определяются для некоторой функции $f(\matr{S})$ как

\begin{gather}
    f^{\star}(\matr{\Lambda}) \stackrel{def}{=} \sup_{\matr{S}}(\trace(\matr{\Lambda}^T\matr{S}) - f(\matr{S})) \text{ --- выпуклая сопряженная}\\
    f_{\star}(\matr{\Lambda}) \stackrel{def}{=} \inf_{\matr{S}}(\trace(\matr{\Lambda}^T\matr{S}) - f(\matr{S})) \text{ --- вогнутая сопряженная}
\end{gather}

Смысл названий этих функций в том, что первая из двух является выпуклой
функцией как поточечний супремум от семейства линейных функций, а вторая как
поточечный инфимум является вогнутой~\cite{boyd_2004}.

С использованием этих определений можем переписать двойственную функцию как

\begin{equation}
    g(\matr{\Lambda}) =
    (f_2)_{\star}(\matr{\Lambda}) - (f_1)^{\star}(\matr{G}^T\matr{\Lambda})
    \label{eq:dual_conjugates}
\end{equation}

Распишем, чему равно выражение \ref{eq:dual_conjugates} для функционала вида \ref{eq:abstract_reg_func_crit}.
В этом случае $f_1(\matr{S}) = \alpha f_{norm}(\matr{S})$, а $f_2(\matr{S}) = - \frac{1}{2}\norm{\matr{X}_{noisy} - \matr{S}}_{fro}^2$.
Действовать будем по аналогии с выводом формулы для разрыва двойственности из статьи \cite{gramfort_2012}.

Воспользуемся правилами вычисления сопряжений для произведения функции на константу~\cite{Boyd}: $(c f)^{\star}(\matr{\Lambda}) = c f^{\star}(\Lambda / c)$.
Получим
\begin{equation}
    (f_1)^{\star}(\matr{G}^T\matr{\Lambda}) =
    \alpha (f_{norm})^{\star}(\matr{G^T\Lambda}/\alpha)
\end{equation}

Выпишем теперь в явном виде, чему равно сопряжение от $f_2$:
\begin{equation*}
    (f_2)^{\star}(\matr{\Lambda}) =
    \inf_{\matr{S}}(\trace(\matr{\Lambda}^T\matr{S}) + \frac{1}{2}\norm{\matr{X}_{noisy} - \matr{S}}_{fro}^2) =
    - \sup_{\matr{S}}(- \trace(\matr{\Lambda}^T\matr{S}) - \frac{1}{2}\norm{\matr{X}_{noisy} - \matr{S}}_{fro}^2)
\end{equation*}
Проведём замену переменной $\matr{Z} = \matr{X}_{noisy} - \matr{S}$:
\begin{equation}
    (f_2)^{\star}(\matr{\Lambda}) =
    \trace(\matr{\Lambda}^T\matr{X}_{noisy}) - \sup_{\matr{S}}(\trace(\matr{\Lambda}^T\matr{Z}) - \frac{1}{2}\norm{\matr{Z}}_{fro}^2)
    \label{eq:conjugate_f_2}
\end{equation}
Супремум в правой части выражения \ref{eq:conjugate_f_2} представляет собой выпуклое сопряжение
от квадрата нормы Фробениуса. Сопряжение половины квадрата нормы равно половине квадрата её двойственной нормы,~\cite{Boyd, gramfort_2012}.
Нормы $L_p$ и $L_{p^{\prime}}$ называются двойственными друг к другу, если для них выполнено $\frac{1}{p} + \frac{1}{p^{\prime}} = 1$.
Норма Фробениуса в соответствии с этим определением является двойственной к самой себе, т.к. по форме совпадает с нормой $L_2$ для векторов, только в
случае матриц берётся их векторизация. Получаем
\begin{equation}
    (f_2)^{\star}(\matr{\Lambda}) =
    \trace(\matr{\Lambda}^T\matr{X}_{noisy}) - \frac{1}{2}\norm{\matr{\Lambda}}^2_{fro}
\end{equation}

Тогда двойственная функция запишется как 
\begin{equation}
    g(\matr{\Lambda}) =
    \trace(\matr{\Lambda}^T\matr{X}_{noisy}) - \frac{1}{2}\norm{\matr{\Lambda}}^2_{fro} - \alpha (f_{norm})^{\star}(\matr{G^T\Lambda}/\alpha)
    \label{eq:dual_func}
\end{equation}
Для нормы без степени сопряженная функция равна индикаторной функции единичного шара для двойственной нормы.
В случае $f_{norm} = \norm{\cdot}_{L_{21}}$, сопряженная функция будет выглядеть как (\cite{gramfort_2012}):

\begin{equation}
    (\norm{\cdot}_{L_{21}})^{\star}(\matr{\Lambda}) = I_{\{\matr{\Lambda} : \norm{\matr{\Lambda}}_{L_{2,\infty}} \leq 1\}}(\matr{\Lambda})=
    \begin{cases}
        0, & \norm{\matr{\Lambda}}_{L_{2,\infty}} \leq 1\\
        \infty, & \norm{\matr{\Lambda}}_{L_{2,\infty}} > 1
    \end{cases}
\end{equation}

Для выражения~\ref{eq:dual_func} наличие индикатора означает, что для
определённых значений двойственной переменной двойственная функция будет
принимать значение $-\infty$.
На практике, чтобы разрыв двойственности
принимал значения в более разумном диапазоне, можем спроецировать целевую переменную
двойственной функции на множество допустимых значений --- единичный шар при
норме $L_{2,\infty}$ (\cite{gramfort_2012, gramfort_2014}).
Эта эвристика основана на том соображении, что
Рассмотренная нами задача \ref{eq:dual_crit_split} является выпуклой и содержит только
ограничения-равенства, а значит для нее выполнены условия Слейтера (при
условии, что области определения функций $f_1$ и $f_2$ имеют непустое
пересечение), и следовательно для нее имеет место сильная двойственность,
т.е. 

\begin{equation}
    \inf_{\matr{S}, \matr{V}}(f_1(\matr{S}) - f_2(\matr{V})) = \sup_{\matr{\Lambda}}(g(\matr{\Lambda})) =
    \sup_{\matr{\Lambda}}((f_2)_{\star}(\matr{\Lambda}) - (f_1)^{\star}(\matr{G}^T\matr{\Lambda})) = \tilde{p},
\end{equation}
где $\tilde{p}$ --- значение исходного функционала в точке минимума.
Поэтому для сходящейся последовательности
значений прямой переменной значения двойственная переменной также сходятся,
а значит и проекции двойственной переменной на выпуклое множество также должны сходиться.

С использованием такой эвристики получим для двойственной функции выражение
\begin{gather}
    g(\matr{\Lambda}) =
    \trace(\matr{\bar{\Lambda}}^T\matr{X}_{noisy}) - \frac{1}{2}\norm{\matr{\bar{\Lambda}}}^2_{fro},\\
    % \label{eq:dual_func_final}
    \matr{\bar{\Lambda}}^{(k)} = \matr{\Lambda}^{(k)} / \max\left(\norm{\matr{G}^T\matr{\Lambda}^{(k)}/\alpha}_{L_{2,\infty}}, 1\right)
    \label{eq:projection_of_dual_var}
\end{gather}
% ----------------------------------------------------------- %

\paragraph{Получим теперь выражение для двойственной переменной через прямую.}


Обозначим
$\tilde{\matr{\Lambda}}$ оптимальное значение двойственной переменной.
Оптимальное значение исходного функционала достигается в точке минимума
функции Лагранжа с зафиксированной $\Lambda = \tilde{\Lambda}$, т.е.
$ \tilde{p} = \inf_{\matr{S}, \matr{V}}(L(\matr{S}, \matr{V}, \tilde{\matr{\Lambda}})) =
g(\tilde{\matr{\Lambda}})$. Тода для матриц $\tilde{\matr{S}},
\tilde{\matr{V}}$, минимизирующих исходный функционал, с учётом $\tilde{\matr{V}} =
\matr{G}\tilde{\matr{S}}$ должно быть выполнено

\begin{align}
    f_1(\tilde{\matr{S}}) + (f_1)^{\star}(\matr{G}^T\tilde{\matr{\Lambda}}) = \trace((\matr{G}^T\tilde{\matr{\Lambda}})^T\tilde{\matr{S}}) \label{eq:dual_kkt_conditions_1}\\
    f_2(\matr{G}\tilde{\matr{S}}) + (f_2)_{\star}(\tilde{\matr{\Lambda}}) = \trace(\tilde{\matr{\Lambda}}^T\matr{G}\tilde{\matr{S}}) \label{eq:dual_kkt_conditions_2}
\end{align}

Эти соотношения представляют условие стационарности функции лагранжа в точке минимума.
В нашем случае второе из этих соотношений запишется как

\begin{equation*}
    - \frac{1}{2} \norm{\matr{X}_{noisy} - \matr{G}\tilde{\matr{S}}}^2_{fro} + \trace({\tilde{\matr{\Lambda}}^T\matr{X}_{noisy}}) - \frac{1}{2}\norm{\tilde{\matr{\Lambda}}}^2_{fro}=
    \trace(\tilde{\matr{\Lambda}}^T\matr{G}\tilde{\matr{S}})
\end{equation*}
Или эквивалентно

\begin{multline*}
    - \frac{1}{2}\left(\norm{\matr{X}_{noisy}-\matr{G}\tilde{\matr{S}}}^2_{fro} -
        2\trace({\tilde{\matr{\Lambda}}^T(\matr{X}_{noisy}-\matr{G}\tilde{\matr{S}})}) +
    \norm{\tilde{\matr{\Lambda}}}^2_{fro}\right) =\\
    =-\frac{1}{2}\left(\norm{\matr{X}_{noisy} - \matr{G}\tilde{\matr{S}} - \tilde{\matr{\Lambda}}}^2_{fro}\right)
    =0
\end{multline*}
А значит справедливо
\begin{equation}
    \tilde{\matr{\Lambda}} = \matr{X}_{noisy} - \matr{G}\tilde{\matr{S}},
    \label{eq:dual_variable}
\end{equation}
что задает связь между прямой и двойственной переменными. Отметим, что двойственную
переменную можно интерпретировать как ошибку в объяснении измерений при помощи
найденного решения $\hat{\matr{S}}$.

Формулы~\ref{eq:projection_of_dual_var},~\ref{eq:dual_variable} задают
конкреный способ расчёта значения двойственного функционала на каждой итерации
по значению прямой переменной. Разность значения оптимизируемого функционала
на текущей итерации и соответствующего ему значения двойственного функционала
задают разрыв двойственности, который в силу свойства сильной двойственности,
выполненного для нашей задачи, должен сходиться к нулю и следовательно может быть
использован в критерии останова алгоритма.


% \paragraph{Сформулируем итоговый алгоритм минимизации функционала со смешанной нормой и контролем сходимости.}
% Используя формулы~\ref{eq:mm_next_iter_prox},~\ref{eq:dual_func_final},~\ref{eq:projection_of_dual_var},~\ref{eq:dual_variable},
% получим
% Рассмотрим теперь, как будут выглядеть эти условия для обратной задачи с регуляризатором

% \paragraph{} -- highlight first words of a paragraph.

\subsubsection{Ускорение схемы ISTA} 
\label{sec:ista_enhancements}

На практике однако оказывается, что метод
ISTA является не самым эффективным способом оптимизации функционала качества
для спарсных норм. Было показано, что его скорость сходимости имеет порядок
$O(1/K)$, где $K$ --- количество итераций. В 2009 авторы статьи
\cite{beck_trebulle_2009} предложили ускоренную итерационную схему на основе
алгоритма ISTA, которая получила название Fast ISTA или FISTA, и имеет порядок
$O(1/K^2)$. Позднее в статьях \cite{rakotomamonjy_2011} и \cite{qin_2013} для
оптимизации функционала со смешанной нормой $L_{21}$ было предложено
использовать метод блочно-координатного градиентного спуска (block-coordinate
descent, BCD), которая на практике показывает еще более высокие скорости сходимости,
чем алгоритм FISTA.

Суть подхода, основанного на блочно-координатном спуске заключается в следующем.
Полученный в разделе~\ref{subsubsec:ista} алгоритм ISTA на каждой
итерации совершает шаг градиентного спуска с постоянным параметром $1/\beta$,
одинаковым для всех источников. Иными словами, все компоненты матрицы 
оценки источников на $k$-ом шаге $\matr{\hat{S}}_k$ обновляются одновременно.
Метод блочно-координатного спуска состоит в том, чтобы обновлять элементы матрицы $\matr{\hat{S}}_k$
группами. Каждая группа при этом состоит из элементов одной строки матрицы $\matr{\hat{S}}_k$.
Так как структура оператора приближения для нормы $L_{21}$ позволяет
рассчитывать его для каждой такой ``подитерации'' отдельно, обновление переменных из
каждого блока происходит как и для схемы ISTA в явном виде:

\begin{equation}
    \hat{\matr{S}}_i^{(k+1)} = \prox_{\alpha\mu(i)\norm{\cdot}_{L_{2}}} \left(\hat{\matr{S}}^{(k)}_i + \mu(i) \matr{G}_i^T(\matr{X}_{noisy} - \matr{G}\hat{\matr{S}}^{(k)})\right)
    \label{eq:bcd_step}
\end{equation}

Здесь мы используем оператор приближения для нормы $L_2$, а не для нормы $L_{21}$,
так как за раз мы обновляем только одну строку матрицы $\hat{\matr{S}}$, и для одной строки штрафующая норма
будет как раз $L_2$.
В качестве шага градиентного спуска $\mu(i)$ используем $1/\norm{\matr{G}^T_i\matr{G}_i}_{L_2}$,
где $\norm{\matr{G}^T_i\matr{G}_i}_{L_2}$ --- константа Липшица для градиента по $i$-му блоку матрицы $\hat{\matr{S}}$.

Оператор приближения $\prox_{\alpha\mu(i)\norm{\cdot}_{L_{2}}}$ запишется аналогично формуле~\ref{eq:proximity_operator_for_l21}:

\begin{equation}
    \prox_{\alpha\mu(i)\norm{\cdot}_{L_{2}}}(\vec{s}) = \left(1 - \frac{\mu(i)\alpha}{\norm{\vec{s}}_{L_2}}\right)^+\vec{s}
    \label{eq:proximity_bcd}
\end{equation}

На практике при вычислении оператора приближения~\ref{eq:proximity_bcd} следует
учитывать, что какие-то блоки матрицы $\hat{\matr{S}}$ будут равны нулю. В этом
случае формально оператор положительного среза $(\cdot)^+$ будет давать
ожидаемое нулевое значение, однако для практических вычислений более удобно
пользоваться изменённым видом формулы~\ref{eq:proximity_bcd}:
\begin{equation}
    \prox_{\alpha\mu(i)\norm{\cdot}_{L_{2}}}(\vec{s}) = \left(1 - \frac{\mu(i)\alpha}{\max\left(\norm{\vec{s}}_{L_2}, \mu(i)\alpha\right)}\right)\vec{s}
    \label{eq:proximity_bcd_refined}
\end{equation}
Такая формулировка лучше для программирования алгоритма, так как позволяет избежать деления на ноль.

\paragraph{Для дополнительного ускорения расчётов воспользуемся методом активного множества.}
Метод блочно-координатного спуска как правило применяют поочерёдно для всех строк матрицы
$\hat{\matr{S}}$ в рамках одной ``глобальной'' итерации. Но для обратной задачи МЭЭГ
количество строк $\hat{\matr{S}}$ равно количеству источников на коре, поэтому такое
обновление может занять продолжительное время. Вместе с тем из-за свойств спарсной
нормы большая часть строк матрицы $\hat{\matr{S}}$ должна оставаться нулевой. 
Это соображение можно использовать, чтобы обновлять строки матрицы $\hat{\matr{S}}$ исходя из
данных на текущей итерации.

Одним из способов это сделать является метод активного множества.
Этот метод основан на выделении подгруппы ``плохих'' источников
и применении блочно-координатного спуска только на этой подгруппе.
Плохими считаются источники, топографии которых наиболее сильно скоррелированы
с остаточной ошибкой на текущем шаге. Если после оптимизации на этой подгруппе
решение всё ещё не является оптимальным для полной задачи по критерию разрыва двойственности,
активное множество обновляется и процедура повторяется но уже с частью рассчитанных значениями для
строк матрицы $\hat{\matr{S}}$.

Обновление активного множества включает две процедуры:
\begin{enumerate}
    \item удаление из множества источников, которые оказались нулевыми на текущем шаге оптимизации
    \item добавление к множеству новых источников, для которых топография коррелирует с остаточной ошибкой.
\end{enumerate}

Процедура выбора плохих источников неслучайна.
Рассмотрим первое условие стационарности лагранжиана в точке минимума~\ref{eq:dual_kkt_conditions_1}.
Для $f_1(\matr{A}) = \alpha\norm{\matr{A}}_{L_{21}}$ оно запишется как

\begin{equation}
    \alpha \norm{\hat{\matr{S}}}_{L_{21}} + \alpha I_{\{\matr{S} : \norm{\matr{S}}_{L_{2,\infty}} \leq 1\}}(\matr{G}^T\matr{\Lambda}/\alpha) =
    \trace((\matr{G}^T\matr{\Lambda})^T\hat{\matr{S}}).
    \label{eq:dual_kkt_conditions_1_concrete}
\end{equation}
Здесь $\alpha I_{\{\matr{S} : \norm{\matr{S}}_{L_{2,\infty}} \leq 1\}}(\matr{S})$ --- индикатор единичного шара в пространстве
с нормой $L_{2,\infty}$.

Наличие индикатора эквивалентно требованию
\[\max_{i}(\matr{G}^T_i\matr{\Lambda})\leq\alpha,\]
или, что то же самое, для всех $i$ должно выполняться
\begin{equation}
    \matr{G}^T_i\matr{\Lambda}\leq\alpha
    \label{eq:g_lambda_condition}
\end{equation}
Если это условие выполнено, индикатор равен нулю, и выражение
\ref{eq:dual_kkt_conditions_1_concrete} можно расписать как
\begin{equation}
    \alpha \norm{\hat{\matr{S}}}_{L_{21}} =
    \trace((\matr{G}^T\matr{\Lambda})^T\hat{\matr{S}}) \iff
    \alpha \sum_i \norm{\hat{\matr{S}}_i}_{L_2} =
    \sum_i (\matr{G}^T_i\matr{\Lambda})^T\hat{\matr{S}}_i
    \label{eq:intermediate_active_set}
\end{equation}
Правая часть в выражении \ref{eq:intermediate_active_set} представляет собой суммы скалярных
произведений.
Пользуясь \ref{eq:g_lambda_condition} и неравенством Коши-Буняковского-Шварца,
получим
\begin{equation}
   \alpha \sum_i \norm{\hat{\matr{S}}_i}_{L_2} \leq
   \sum_i \norm{\matr{G}^T_i\matr{\Lambda}}_{L_2}\norm{\hat{\matr{S}}_i}_{L_2} \leq
   \alpha \sum_i \norm{\hat{\matr{S}}_i}_{L_2}
\end{equation}
Последнее выполнено тогда и только тогда, когда для каждого индекса $i$
либо $\norm{\hat{\matr{S}}_i}_{L_2} = 0$ и $\norm{\matr{G}^T_i\matr{\Lambda}}_{L_2} \leq \alpha$,
либо $\norm{\hat{\matr{S}}_i}_{L_2} \neq 0$ и $\norm{\matr{G}^T_i\matr{\Lambda}}_{L_2} = \alpha$.
Т.е. в точке оптимума для всех ненулевых источников проекция двойственной переменной на соответствующие
топографии не длиннее $\alpha$, а для ненулевых длина такой проекции в точности равна $\alpha$.

В обратную сторону утверждение также верно. Если для строк некоторой матрицы $\matr{S}$ выполнено, что
либо $\norm{\matr{S}_i}_{L_2} = 0$ и $\norm{\matr{G}^T_i(\matr{X} _{noisy}- \matr{GS})}_{L_2} \leq \alpha$,
либо $\norm{\matr{S}_i}_{L_2} \neq 0$ и $\norm{\matr{G}^T_i(\matr{X}_{noisy} - \matr{GS})}_{L_2} = \alpha$,
то эта точка является точкой минимума нашего функционала.

Это наблюдение позволяет выделить из всех источников те, которые с большей вероятностью не равны нулю,
и обновлять в блочно-координатном спуске только их. Отбор индексов источников $i$, которые необходимо
оптимизировать в первую очередь, при этом происходит
по критерию $\norm{\matr{G}_i^T(\matr{M} - \matr{G}\matr{S})}_{L_2} > \alpha$.

Отметим, что этот критерий весьма близок по духу к алгоритму RAP-MUSIC~\ref{subsection:rap_music}.
В алгоритме RAP-MUSIC мы искали источник, топография которого сильнее других коррелируют с подпространством сигнала,
из которого удален вклад источников, найденных на предыдущих итерациях. Здесь мы для обновления
активного множества делаем то же самое за исключением выделения подпространства сигнала ---
корреляция топографии считается напрямую с остаточной нормой ошибки.

Сформулируем алгоритм оптимизации блочно-координатным спуском с использованием
активного множества.
На первом шаге необходимо инициализировать массив
$\hat{\matr{S}}^{(1)}$ нулями. Далее найдём набор $A$, состоящий из $l$
источников, которые наиболее сильно нарушают условие
$\norm{\matr{G}_i^T(\matr{M} - \matr{G}\matr{S})}_{L_2} > \alpha$.
Для матрицы
$\matr{G}_{A}$ --- ограничения $\matr{G}$ на $A$ --- и $\hat{\matr{S}}_{A}$ --- ограничения
$\hat{\matr{S}}$ на $A$ найдём оптимальное решение для функционала \ref{eq:l21_reg_func_crit}
методом блочно-координатного спуска. В качестве критерия оптимальности используем
разрыв двойственности для ограниченной задачи. Строкам $\hat{\matr{S}}$, соответствующим
элементам множества $A$, присвоим найденные значения $\matr{S}_A$. Если
полученная матрица $\hat{\matr{S}}$ является оптимальной по критерию разрыва двойственности,
алгоритм останавливается. Если нет, дополняем множество $A$ l новыми источниками,
наиболее сильно нарушающими условие 
$\norm{\matr{G}_i^T(\matr{M} - \matr{G}\matr{S})}_{L_2} > \alpha$ и повторяем алгоритм для
нового множества $A$.

\subsubsection{Метод смешанной нормы с итеративным перевзвешиванием.}
Оказывается, что на основе метода \ref{sec:ista} % TODO
можно сформулировать алгоритм, который аппроксимирует штрафующую норму $L_0$ точнее,
тем самым усиливая спарсность получаемых решений и позволяя избавиться
от некоторых негативных свойств алгоритма с нормой $L_{1}$, в число в которых
входит смещённость оценки амплитуды источноков.

Авторы статьи \cite{gramfort_2014} предложили использовать в качестве штрафующей
функции смешанную псевдонорму $L_{2, 0.5}$. Эта псевдонорма обладает более
крутым градиентом вблизи нуля и тем самым лучше моделирует форму индикаторной функции,
которая используется при построении алгоритма с $L_0$-штрафом.

Тем не менее, использование псевдонормы $L_{2, 0.5}$ имеет свою цену: эта псевдонорма
не является выпуклой, что усложняет задачу численной оптимизации целевого функционала.

Тем не менее, в работах ~\cite{8, 9, 13 from gramfort_2014} было показано, что
итоговый невыпуклый функционал можно оптимизировать, разбив оптимизационную
процедуру на подитерации, на каждой из которых оптимизируется выпуклый
функционал с нормой $L_{21}$. Рассмотрим более подробно суть этого подхода.


Функционал качества с нормой $L_{2, 0.5}$ может быть записан как

\begin{equation}
    \frac{1}{2}\norm{\matr{X}_{noisy} - \matr{G}\matr{S}}^2_{fro} + \alpha \sum_i{\sqrt{\norm{\matr{S}_i}_{L2}}}
    \rightarrow \underset{\matr{S}}{\min}.
    \label{eq:l205_crit}
\end{equation}

Главную проблему для оптимизации \ref{eq:l205_crit} представляет квадратный
корень в штрафующей норме. Идея метода последовательного перевзвешивания
заключается в том, что мы аппроксимируем корень, используя веса для строк матрицы
$\matr{S}$ следующим образом:

\begin{equation}
    \hat{\matr{S}}^{(k)} = \argmin_{\matr{S}} \frac{1}{2}\norm{\matr{X}_{noisy}
    - \matr{G}\matr{S}}^2_{fro} + \alpha
    \sum_i{\frac{1}{\sqrt{\norm{\hat{\matr{S}}_i^{(k-1)}}_{L_2}}}\norm{\matr{S}_i}_{L2}}
\label{eq:l205_crit_subiter}
\end{equation}
Для $k$-го шага значения $1/\sqrt{\norm{\hat{\matr{S}}_i^{(k-1)}}_{L_2}}$ рассчитываются
на основе решения, найденного на шаге $k-1$, а значит являются константными. Обозначим
величины $\sqrt{\norm{\hat{\matr{S}}_i^{(k-1)}}_{L_2}}$ как $w^{(k)}$.

Задача~\ref{eq:l205_crit_subiter} может быть эквивалентно сформулирована как
\begin{multline}
    \hat{\matr{S}}^{(k)} = \argmin_{\matr{S}} \frac{1}{2}\norm{\matr{X}_{noisy}
    - \matr{G}\matr{W}^{(k)}\matr{S}}^2_{fro} + \alpha
    \sum_i{\norm{\matr{S}_i}_{L2}} = \\
    = \argmin_{\matr{S}} \frac{1}{2}\norm{\matr{X}_{noisy}
    - \matr{G}^{(k)}\matr{S}}^2_{fro} + \alpha
    \sum_i{\norm{\matr{S}_i}_{L2}}
\label{eq:l205_crit_subiter_Wk}
\end{multline}
Здесь $\matr{W}^{(k)} = \diag(w^{(k)})$, a $\matr{G}^{(k)} = \matr{G} \matr{W}^{(k)}$.

Как видим, оптимизационная задача для подитерации свелась к задаче~\ref{eq:l21_reg_func_crit}.
Решение для каждой подитерации теперь может быть получено методом, описанным в разделе
\ref{subsubsec:ista} с техниками ускорения расчета из раздела \ref{sec:ista_enhancements}.
Найденные последовательные приближения $\matr{\hat{S}}^{(k)}$ сходятся к локальному минимуму
функционала \ref{eq:l205_crit}.

Так как исходный функционал не являлся выпуклым, сходимость такого метода к глобальному минимуму
функционала \ref{eq:l205_crit} не гарантирована и может зависеть от выбора начального приближения
для весов $w^{(k)}$.
